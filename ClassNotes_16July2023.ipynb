{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Merged Jupyter Notebook*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><font color=\"green\"><h1>from file: 01-TokenizingTextIntoSentencesAndWords</h1></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize text into words and sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16.1\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "print(numpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uses NLTK's recommended sentence tokenizer, the PunktSentenceTokenizer\n",
    "#### Uses NLTK's recommended word tokenizer, the TreebankWordTokenizer and the PunktSentencetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/loonycorn/nltk_data'\n    - '/anaconda3/nltk_data'\n    - '/anaconda3/share/nltk_data'\n    - '/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-27d5e78d663c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msent_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Does this tokenizer work? These are two different sentences'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \"\"\"\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    697\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/loonycorn/nltk_data'\n    - '/anaconda3/nltk_data'\n    - '/anaconda3/share/nltk_data'\n    - '/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "sent_tokens = sent_tokenize('Does this tokenizer work? These are two different sentences')\n",
    "\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/loonycorn/nltk_data'\n    - '/anaconda3/nltk_data'\n    - '/anaconda3/share/nltk_data'\n    - '/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-42e4b9771e48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Does this tokenizer work?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \"\"\"\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     return [\n\u001b[1;32m    145\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \"\"\"\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    697\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/loonycorn/nltk_data'\n    - '/anaconda3/nltk_data'\n    - '/anaconda3/share/nltk_data'\n    - '/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize('Does this tokenizer work?')\n",
    "\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Punkt tokenizer\n",
    "\n",
    "https://www.nltk.org/_modules/nltk/tokenize/punkt.html\n",
    "http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.punkt\n",
    "\n",
    "A sentence tokenizer which uses an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences; and then uses that model to find sentence boundaries. This approach has been shown to work well for many European languages.\n",
    "\n",
    "It must be trained on a large collection of plaintext in the target language before it can be used.\n",
    "\n",
    "The NLTK data package includes a pre-trained Punkt tokenizer for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/loonycorn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Does this tokenizer work?', 'These are two different sentences']\n"
     ]
    }
   ],
   "source": [
    "sent_tokens = sent_tokenize('Does this tokenizer work? These are two different sentences')\n",
    "\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Does', 'this', 'tokenizer', 'work', '?']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize('Does this tokenizer work?')\n",
    "\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'bird', 'in', 'hand', 'is', 'worth', 'two', 'in', 'the', 'bush', '.', 'Good', 'things', 'come', 'to', 'those', 'who', 'wait', '.', 'These', 'watches', 'cost', '$', '1500', '!', 'The', 'ball', 'is', 'in', 'your', 'court', '.', 'Mr.', 'Smith', 'Goes', 'to', 'Washington', 'Doogie', 'Howser', 'M.D', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"A bird in hand is worth two in the bush. \" +\\\n",
    "       \"Good things come to those who wait. \" +\\\n",
    "       \"These watches cost $1500! \" +\\\n",
    "       \"The ball is in your court. \" +\\\n",
    "       \"Mr. Smith Goes to Washington \" +\\\n",
    "       \"Doogie Howser M.D.\"\n",
    "\n",
    "word_tokens = word_tokenize(text, language='english')\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hand', 'is', 'worth', 'two', 'in']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens[3:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = PunktSentenceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A bird in hand is worth two in the bush.', 'Good things come to those who wait.', 'These watches cost $1500!', 'The ball is in your court.', 'Mr.', 'Smith Goes to Washington Doogie Howser M.D.']\n"
     ]
    }
   ],
   "source": [
    "sent_tokens = pst.tokenize(text)\n",
    "\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 40), (41, 76), (77, 102), (103, 129), (130, 133), (134, 177)]\n"
     ]
    }
   ],
   "source": [
    "span_tokens = pst.span_tokenize(text)\n",
    "\n",
    "print(list(span_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A', 'bird', 'in', 'hand', 'is', 'worth', 'two', 'in', 'the', 'bush', '.'],\n",
       " ['Good', 'things', 'come', 'to', 'those', 'who', 'wait', '.'],\n",
       " ['These', 'watches', 'cost', '$', '1500', '!'],\n",
       " ['The', 'ball', 'is', 'in', 'your', 'court', '.'],\n",
       " ['Mr.'],\n",
       " ['Smith', 'Goes', 'to', 'Washington', 'Doogie', 'Howser', 'M.D', '.']]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = pst.sentences_from_tokens(word_tokens)\n",
    "\n",
    "list(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'bird', 'in', 'hand', 'is', 'worth', 'two', 'in', 'the', 'bush.', 'Good', 'things', 'come', 'to', 'those', 'who', 'wait.', 'These', 'watches', 'cost', '$1500!', 'The', 'ball', 'is', 'in', 'your', 'court.', 'Mr.', 'Smith', 'Goes', 'to', 'Washington', 'Doogie', 'Howser', 'M.D.']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = wt.tokenize(text)\n",
    "\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Local Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marie Curie was a Polish-born physicist and chemist and one of the most famous scientists of her time.\n",
      "Together with her husband Pierre, she was awarded the Nobel Prize in 1903, and she went on to win another in 1911.\n",
      "Marie Sklodowska was born in Warsaw on 7 November 1867, the daughter of a teacher.\n",
      "In 1891, she went to Paris to study physics and mathematics at the Sorbonne where she met Pierre Curie, professor of the School of Physics.\n",
      "They were married in 1895.\n",
      "The Curies worked together investigating radioactivity, building on the work of the German physicist Roentgen and the French physicist Becquerel.\n",
      "In July 1898, the Curies announced the discovery of a new chemical element, polonium.\n",
      "At the end of the year, they announced the discovery of another, radium.\n",
      "The Curies, along with Becquerel, were awarded the Nobel Prize for Physics in 1903.\n",
      "Pierre's life was cut short in 1906 when he was knocked down and killed by a carriage.\n",
      "Marie took over his teaching post, becoming the first woman to teach at the Sorbonne, and devoted herself to continuing the work that they had begun together.\n",
      "She received a second Nobel Prize, for Chemistry, in 1911.\n",
      "The Curie's research was crucial in the development of x-rays in surgery.\n",
      "During World War One Curie helped to equip ambulances with x-ray equipment, which she herself drove to the front lines.\n",
      "The International Red Cross made her head of its radiological service and she held training courses for medical orderlies and doctors in the new techniques.\n",
      "Despite her success, Marie continued to face great opposition from male scientists in France, and she never received significant financial benefits from her work.\n",
      "By the late 1920s her health was beginning to deteriorate.\n",
      "She died on 4 July 1934 from leukaemia, caused by exposure to high-energy radiation from her research.\n",
      "The Curies' eldest daughter Irene was herself a scientist and winner of the Nobel Prize for Chemistry.\n"
     ]
    }
   ],
   "source": [
    "with open('./datasets/biography.txt', 'r') as f:\n",
    "    file_contents = f.read()\n",
    "\n",
    "print(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Marie', 'Curie', 'was', 'a', 'Polish-born', 'physicist', 'and', 'chemist', 'and', 'one', 'of', 'the', 'most', 'famous', 'scientists', 'of', 'her', 'time', '.', 'Together', 'with', 'her', 'husband', 'Pierre', ',', 'she', 'was', 'awarded', 'the', 'Nobel', 'Prize', 'in', '1903', ',', 'and', 'she', 'went', 'on', 'to', 'win', 'another', 'in', '1911', '.', 'Marie', 'Sklodowska', 'was', 'born', 'in', 'Warsaw', 'on', '7', 'November', '1867', ',', 'the', 'daughter', 'of', 'a', 'teacher', '.', 'In', '1891', ',', 'she', 'went', 'to', 'Paris', 'to', 'study', 'physics', 'and', 'mathematics', 'at', 'the', 'Sorbonne', 'where', 'she', 'met', 'Pierre', 'Curie', ',', 'professor', 'of', 'the', 'School', 'of', 'Physics', '.', 'They', 'were', 'married', 'in', '1895', '.', 'The', 'Curies', 'worked', 'together', 'investigating', 'radioactivity', ',', 'building', 'on', 'the', 'work', 'of', 'the', 'German', 'physicist', 'Roentgen', 'and', 'the', 'French', 'physicist', 'Becquerel', '.', 'In', 'July', '1898', ',', 'the', 'Curies', 'announced', 'the', 'discovery', 'of', 'a', 'new', 'chemical', 'element', ',', 'polonium', '.', 'At', 'the', 'end', 'of', 'the', 'year', ',', 'they', 'announced', 'the', 'discovery', 'of', 'another', ',', 'radium', '.', 'The', 'Curies', ',', 'along', 'with', 'Becquerel', ',', 'were', 'awarded', 'the', 'Nobel', 'Prize', 'for', 'Physics', 'in', '1903', '.', 'Pierre', \"'s\", 'life', 'was', 'cut', 'short', 'in', '1906', 'when', 'he', 'was', 'knocked', 'down', 'and', 'killed', 'by', 'a', 'carriage', '.', 'Marie', 'took', 'over', 'his', 'teaching', 'post', ',', 'becoming', 'the', 'first', 'woman', 'to', 'teach', 'at', 'the', 'Sorbonne', ',', 'and', 'devoted', 'herself', 'to', 'continuing', 'the', 'work', 'that', 'they', 'had', 'begun', 'together', '.', 'She', 'received', 'a', 'second', 'Nobel', 'Prize', ',', 'for', 'Chemistry', ',', 'in', '1911', '.', 'The', 'Curie', \"'s\", 'research', 'was', 'crucial', 'in', 'the', 'development', 'of', 'x-rays', 'in', 'surgery', '.', 'During', 'World', 'War', 'One', 'Curie', 'helped', 'to', 'equip', 'ambulances', 'with', 'x-ray', 'equipment', ',', 'which', 'she', 'herself', 'drove', 'to', 'the', 'front', 'lines', '.', 'The', 'International', 'Red', 'Cross', 'made', 'her', 'head', 'of', 'its', 'radiological', 'service', 'and', 'she', 'held', 'training', 'courses', 'for', 'medical', 'orderlies', 'and', 'doctors', 'in', 'the', 'new', 'techniques', '.', 'Despite', 'her', 'success', ',', 'Marie', 'continued', 'to', 'face', 'great', 'opposition', 'from', 'male', 'scientists', 'in', 'France', ',', 'and', 'she', 'never', 'received', 'significant', 'financial', 'benefits', 'from', 'her', 'work', '.', 'By', 'the', 'late', '1920s', 'her', 'health', 'was', 'beginning', 'to', 'deteriorate', '.', 'She', 'died', 'on', '4', 'July', '1934', 'from', 'leukaemia', ',', 'caused', 'by', 'exposure', 'to', 'high-energy', 'radiation', 'from', 'her', 'research', '.', 'The', 'Curies', \"'\", 'eldest', 'daughter', 'Irene', 'was', 'herself', 'a', 'scientist', 'and', 'winner', 'of', 'the', 'Nobel', 'Prize', 'for', 'Chemistry', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(file_contents)\n",
    "\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 182 samples and 367 outcomes>\n"
     ]
    }
   ],
   "source": [
    "freq_dist = FreqDist(word_tokens)\n",
    "\n",
    "print(freq_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 22),\n",
       " (',', 20),\n",
       " ('.', 19),\n",
       " ('of', 12),\n",
       " ('and', 11),\n",
       " ('in', 11),\n",
       " ('to', 10),\n",
       " ('was', 8),\n",
       " ('her', 7),\n",
       " ('she', 7),\n",
       " ('a', 6),\n",
       " ('The', 5),\n",
       " ('Marie', 4),\n",
       " ('Curie', 4),\n",
       " ('Nobel', 4),\n",
       " ('Prize', 4),\n",
       " ('on', 4),\n",
       " ('Curies', 4),\n",
       " ('for', 4),\n",
       " ('from', 4)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dist.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the frequency of a given sample. The frequency of a sample is defined as the count of that sample divided by the total number of sample outcomes that have been recorded by this FreqDist. The count of a sample is defined as the number of times that sample outcome was recorded by this FreqDist. Frequencies are always real numbers in the range [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05994550408719346"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dist.freq('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0027247956403269754"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dist.freq('exposure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "freq_dist.plot(20, cumulative=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['marie', 'curie', 'was', 'a', 'polish-born', 'physicist', 'and', 'chemist', 'and', 'one', 'of', 'the', 'most', 'famous', 'scientists', 'of', 'her', 'time.', 'together', 'with', 'her', 'husband', 'pierre,', 'she', 'was', 'awarded', 'the', 'nobel', 'prize', 'in', '1903,', 'and', 'she', 'went', 'on', 'to', 'win', 'another', 'in', '1911.', 'marie', 'sklodowska', 'was', 'born', 'in', 'warsaw', 'on', '7', 'november', '1867,', 'the', 'daughter', 'of', 'a', 'teacher.', 'in', '1891,', 'she', 'went', 'to', 'paris', 'to', 'study', 'physics', 'and', 'mathematics', 'at', 'the', 'sorbonne', 'where', 'she', 'met', 'pierre', 'curie,', 'professor', 'of', 'the', 'school', 'of', 'physics.', 'they', 'were', 'married', 'in', '1895.', 'the', 'curies', 'worked', 'together', 'investigating', 'radioactivity,', 'building', 'on', 'the', 'work', 'of', 'the', 'german', 'physicist', 'roentgen', 'and', 'the', 'french', 'physicist', 'becquerel.', 'in', 'july', '1898,', 'the', 'curies', 'announced', 'the', 'discovery', 'of', 'a', 'new', 'chemical', 'element,', 'polonium.', 'at', 'the', 'end', 'of', 'the', 'year,', 'they', 'announced', 'the', 'discovery', 'of', 'another,', 'radium.', 'the', 'curies,', 'along', 'with', 'becquerel,', 'were', 'awarded', 'the', 'nobel', 'prize', 'for', 'physics', 'in', '1903.', \"pierre's\", 'life', 'was', 'cut', 'short', 'in', '1906', 'when', 'he', 'was', 'knocked', 'down', 'and', 'killed', 'by', 'a', 'carriage.', 'marie', 'took', 'over', 'his', 'teaching', 'post,', 'becoming', 'the', 'first', 'woman', 'to', 'teach', 'at', 'the', 'sorbonne,', 'and', 'devoted', 'herself', 'to', 'continuing', 'the', 'work', 'that', 'they', 'had', 'begun', 'together.', 'she', 'received', 'a', 'second', 'nobel', 'prize,', 'for', 'chemistry,', 'in', '1911.', 'the', \"curie's\", 'research', 'was', 'crucial', 'in', 'the', 'development', 'of', 'x-rays', 'in', 'surgery.', 'during', 'world', 'war', 'one', 'curie', 'helped', 'to', 'equip', 'ambulances', 'with', 'x-ray', 'equipment,', 'which', 'she', 'herself', 'drove', 'to', 'the', 'front', 'lines.', 'the', 'international', 'red', 'cross', 'made', 'her', 'head', 'of', 'its', 'radiological', 'service', 'and', 'she', 'held', 'training', 'courses', 'for', 'medical', 'orderlies', 'and', 'doctors', 'in', 'the', 'new', 'techniques.', 'despite', 'her', 'success,', 'marie', 'continued', 'to', 'face', 'great', 'opposition', 'from', 'male', 'scientists', 'in', 'france,', 'and', 'she', 'never', 'received', 'significant', 'financial', 'benefits', 'from', 'her', 'work.', 'by', 'the', 'late', '1920s', 'her', 'health', 'was', 'beginning', 'to', 'deteriorate.', 'she', 'died', 'on', '4', 'july', '1934', 'from', 'leukaemia,', 'caused', 'by', 'exposure', 'to', 'high-energy', 'radiation', 'from', 'her', 'research.', 'the', \"curies'\", 'eldest', 'daughter', 'irene', 'was', 'herself', 'a', 'scientist', 'and', 'winner', 'of', 'the', 'nobel', 'prize', 'for', 'chemistry.']\n"
     ]
    }
   ],
   "source": [
    "file_contents = file_contents.lower()\n",
    "\n",
    "word_tokens = wt.tokenize(file_contents)\n",
    "\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/loonycorn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'an', 'should', \"needn't\", 'o', 'ours', 'yourselves', 'am', 't', 'd', 'our', 'yourself', 'himself', 'wasn', 'above', 'you', 'me', 'did', \"hasn't\", 'shan', 'about', 'wouldn', 'shouldn', 'because', 'myself', 'where', 'hasn', 'hadn', 'own', 'hers', 'at', \"mightn't\", 'now', 'with', 'how', 'some', 'or', 'had', 'having', 'aren', 'm', \"shouldn't\", 'was', 'don', 'same', \"didn't\", 'against', 'why', 'y', 'any', 'do', 'being', 'll', 'up', 'doesn', 'which', 'again', 'as', 'in', 'mightn', \"you'll\", \"that'll\", 'can', 'for', 'haven', 's', 'him', 'ain', 'from', 'then', 'once', 'that', \"you're\", 'such', 'does', '.', 'these', 'be', 'only', 'just', 'will', 'into', 'she', \"aren't\", 'and', 'they', 'isn', 'been', 'until', 'theirs', \"don't\", \"couldn't\", 'we', 'ourselves', 'under', 'is', 'by', 'what', \"hadn't\", \"weren't\", 'too', 'ma', 'needn', 'those', 'are', 'more', 'there', 'to', 'between', 'herself', 'has', 'a', 'the', 'yours', 'who', 'than', 'not', 'of', 'nor', \"isn't\", 'when', 'whom', \"it's\", 'i', 'my', 'were', 'mustn', 'doing', 'down', \"won't\", 'all', 'have', \"you'd\", 'his', 'other', 'but', 'if', 'very', 'off', 'before', 'both', 'each', 'didn', 'further', 'no', 'most', \"doesn't\", 'couldn', 'them', 'through', 'weren', 'won', \"wouldn't\", 'The', 're', \"wasn't\", 'few', 'here', 'so', ',', 'its', 'their', 'below', \"shan't\", 'out', \"mustn't\", 'it', 'over', 'itself', 'while', \"you've\", 'themselves', 'after', 'your', 'her', \"should've\", \"haven't\", 'this', 'on', 've', \"she's\", 'he', 'during'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['.', ',', 'The'])\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['marie', 'curie', 'polish-born', 'physicist', 'chemist', 'one', 'famous', 'scientists', 'time.', 'together', 'husband', 'pierre,', 'awarded', 'nobel', 'prize', '1903,', 'went', 'win', 'another', '1911.', 'marie', 'sklodowska', 'born', 'warsaw', '7', 'november', '1867,', 'daughter', 'teacher.', '1891,', 'went', 'paris', 'study', 'physics', 'mathematics', 'sorbonne', 'met', 'pierre', 'curie,', 'professor', 'school', 'physics.', 'married', '1895.', 'curies', 'worked', 'together', 'investigating', 'radioactivity,', 'building', 'work', 'german', 'physicist', 'roentgen', 'french', 'physicist', 'becquerel.', 'july', '1898,', 'curies', 'announced', 'discovery', 'new', 'chemical', 'element,', 'polonium.', 'end', 'year,', 'announced', 'discovery', 'another,', 'radium.', 'curies,', 'along', 'becquerel,', 'awarded', 'nobel', 'prize', 'physics', '1903.', \"pierre's\", 'life', 'cut', 'short', '1906', 'knocked', 'killed', 'carriage.', 'marie', 'took', 'teaching', 'post,', 'becoming', 'first', 'woman', 'teach', 'sorbonne,', 'devoted', 'continuing', 'work', 'begun', 'together.', 'received', 'second', 'nobel', 'prize,', 'chemistry,', '1911.', \"curie's\", 'research', 'crucial', 'development', 'x-rays', 'surgery.', 'world', 'war', 'one', 'curie', 'helped', 'equip', 'ambulances', 'x-ray', 'equipment,', 'drove', 'front', 'lines.', 'international', 'red', 'cross', 'made', 'head', 'radiological', 'service', 'held', 'training', 'courses', 'medical', 'orderlies', 'doctors', 'new', 'techniques.', 'despite', 'success,', 'marie', 'continued', 'face', 'great', 'opposition', 'male', 'scientists', 'france,', 'never', 'received', 'significant', 'financial', 'benefits', 'work.', 'late', '1920s', 'health', 'beginning', 'deteriorate.', 'died', '4', 'july', '1934', 'leukaemia,', 'caused', 'exposure', 'high-energy', 'radiation', 'research.', \"curies'\", 'eldest', 'daughter', 'irene', 'scientist', 'winner', 'nobel', 'prize', 'chemistry.']\n"
     ]
    }
   ],
   "source": [
    "filtered_words = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_words.append(w)\n",
    "        \n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist = FreqDist(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marie', 4),\n",
       " ('nobel', 4),\n",
       " ('physicist', 3),\n",
       " ('prize', 3),\n",
       " ('curie', 2),\n",
       " ('one', 2),\n",
       " ('scientists', 2),\n",
       " ('together', 2),\n",
       " ('awarded', 2),\n",
       " ('went', 2),\n",
       " ('1911.', 2),\n",
       " ('daughter', 2),\n",
       " ('physics', 2),\n",
       " ('curies', 2),\n",
       " ('work', 2),\n",
       " ('july', 2),\n",
       " ('announced', 2),\n",
       " ('discovery', 2),\n",
       " ('new', 2),\n",
       " ('received', 2)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dist.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "freq_dist.plot(20, cumulative=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><font color=\"green\"><h1>from file: 02-VectorizeTextAsABagOfWords_CountVectorizer</h1></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize text as a bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20.3\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [\"A bird in hand is worth two in the bush.\",\n",
    "              \"Good things come to those who wait.\",\n",
    "              \"These watches cost $1500! \",\n",
    "              \"There are other fish in the sea.\",\n",
    "              \"The ball is in your court.\",\n",
    "              \"Mr. Smith Goes to Washington \",\n",
    "              \"Doogie Howser M.D.\"]\n",
    "\n",
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.fit(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1500',\n",
       " 'are',\n",
       " 'ball',\n",
       " 'bird',\n",
       " 'bush',\n",
       " 'come',\n",
       " 'cost',\n",
       " 'court',\n",
       " 'doogie',\n",
       " 'fish',\n",
       " 'goes',\n",
       " 'good',\n",
       " 'hand',\n",
       " 'howser',\n",
       " 'in',\n",
       " 'is',\n",
       " 'mr',\n",
       " 'other',\n",
       " 'sea',\n",
       " 'smith',\n",
       " 'the',\n",
       " 'there',\n",
       " 'these',\n",
       " 'things',\n",
       " 'those',\n",
       " 'to',\n",
       " 'two',\n",
       " 'wait',\n",
       " 'washington',\n",
       " 'watches',\n",
       " 'who',\n",
       " 'worth',\n",
       " 'your']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bird': 3,\n",
       " 'in': 14,\n",
       " 'hand': 12,\n",
       " 'is': 15,\n",
       " 'worth': 31,\n",
       " 'two': 26,\n",
       " 'the': 20,\n",
       " 'bush': 4,\n",
       " 'good': 11,\n",
       " 'things': 23,\n",
       " 'come': 5,\n",
       " 'to': 25,\n",
       " 'those': 24,\n",
       " 'who': 30,\n",
       " 'wait': 27,\n",
       " 'these': 22,\n",
       " 'watches': 29,\n",
       " 'cost': 6,\n",
       " '1500': 0,\n",
       " 'there': 21,\n",
       " 'are': 1,\n",
       " 'other': 17,\n",
       " 'fish': 9,\n",
       " 'sea': 18,\n",
       " 'ball': 2,\n",
       " 'your': 32,\n",
       " 'court': 7,\n",
       " 'mr': 16,\n",
       " 'smith': 19,\n",
       " 'goes': 10,\n",
       " 'washington': 28,\n",
       " 'doogie': 8,\n",
       " 'howser': 13}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.vocabulary_.get('things')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A bird in hand is worth two in the bush.',\n",
       " 'Good things come to those who wait.',\n",
       " 'These watches cost $1500! ',\n",
       " 'There are other fish in the sea.',\n",
       " 'The ball is in your court.',\n",
       " 'Mr. Smith Goes to Washington ',\n",
       " 'Doogie Howser M.D.']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_vector = count_vectorizer.transform(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 33)\n"
     ]
    }
   ],
   "source": [
    "print(transformed_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0 0 0 0 0 0 0 1 0 2 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0]\n",
      " [1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(transformed_vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = [\"Every cloud has a silver lining.\"]\n",
    "\n",
    "count_vectorizer.transform(test_text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.fit(train_text + test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bird': 3, 'in': 17, 'hand': 14, 'is': 18, 'worth': 36, 'two': 31, 'the': 25, 'bush': 4, 'good': 13, 'things': 28, 'come': 6, 'to': 30, 'those': 29, 'who': 35, 'wait': 32, 'these': 27, 'watches': 34, 'cost': 7, '1500': 0, 'there': 26, 'are': 1, 'other': 21, 'fish': 11, 'sea': 22, 'ball': 2, 'your': 37, 'court': 8, 'mr': 20, 'smith': 24, 'goes': 12, 'washington': 33, 'doogie': 9, 'howser': 16, 'every': 10, 'cloud': 5, 'has': 15, 'silver': 23, 'lining': 19}\n"
     ]
    }
   ],
   "source": [
    "print(count_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.transform(test_text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x38 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"That bird is sitting in the bush and this bird is in hand.\",\n",
    "        \"Wait and then walk\",\n",
    "        \"Watches are cool \"]\n",
    "\n",
    "transformed_vector = count_vectorizer.transform(text)\n",
    "\n",
    "transformed_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t2\n",
      "  (0, 4)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 17)\t2\n",
      "  (0, 18)\t2\n",
      "  (0, 25)\t1\n",
      "  (1, 32)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 34)\t1\n"
     ]
    }
   ],
   "source": [
    "print(transformed_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 38)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 2 1 0 0 0 0 0 0 0 0 0 1 0 0 2 2 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      "  0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      "  0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(transformed_vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer on text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marie Curie was a Polish-born physicist and chemist and one of the most famous scientists of her time.\n",
      "Together with her husband Pierre, she was awarded the Nobel Prize in 1903, and she went on to win another in 1911.\n",
      "Marie Sklodowska was born in Warsaw on 7 November 1867, the daughter of a teacher.\n",
      "In 1891, she went to Paris to study physics and mathematics at the Sorbonne where she met Pierre Curie, professor of the School of Physics.\n",
      "They were married in 1895.\n",
      "The Curies worked together investigating radioactivity, building on the work of the German physicist Roentgen and the French physicist Becquerel.\n",
      "In July 1898, the Curies announced the discovery of a new chemical element, polonium.\n",
      "At the end of the year, they announced the discovery of another, radium.\n",
      "The Curies, along with Becquerel, were awarded the Nobel Prize for Physics in 1903.\n",
      "Pierre's life was cut short in 1906 when he was knocked down and killed by a carriage.\n",
      "Marie took over his teaching post, becoming the first woman to teach at the Sorbonne, and devoted herself to continuing the work that they had begun together.\n",
      "She received a second Nobel Prize, for Chemistry, in 1911.\n",
      "The Curie's research was crucial in the development of x-rays in surgery.\n",
      "During World War One Curie helped to equip ambulances with x-ray equipment, which she herself drove to the front lines.\n",
      "The International Red Cross made her head of its radiological service and she held training courses for medical orderlies and doctors in the new techniques.\n",
      "Despite her success, Marie continued to face great opposition from male scientists in France, and she never received significant financial benefits from her work.\n",
      "By the late 1920s her health was beginning to deteriorate.\n",
      "She died on 4 July 1934 from leukaemia, caused by exposure to high-energy radiation from her research.\n",
      "The Curies' eldest daughter Irene was herself a scientist and winner of the Nobel Prize for Chemistry.\n"
     ]
    }
   ],
   "source": [
    "with open('./datasets/biography.txt', 'r') as f:\n",
    "    file_contents = f.read()\n",
    "\n",
    "print(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Marie Curie was a Polish-born physicist and chemist and one of the most famous scientists of her time.', 'Together with her husband Pierre, she was awarded the Nobel Prize in 1903, and she went on to win another in 1911.', 'Marie Sklodowska was born in Warsaw on 7 November 1867, the daughter of a teacher.', 'In 1891, she went to Paris to study physics and mathematics at the Sorbonne where she met Pierre Curie, professor of the School of Physics.', 'They were married in 1895.', 'The Curies worked together investigating radioactivity, building on the work of the German physicist Roentgen and the French physicist Becquerel.', 'In July 1898, the Curies announced the discovery of a new chemical element, polonium.', 'At the end of the year, they announced the discovery of another, radium.', 'The Curies, along with Becquerel, were awarded the Nobel Prize for Physics in 1903.', \"Pierre's life was cut short in 1906 when he was knocked down and killed by a carriage.\", 'Marie took over his teaching post, becoming the first woman to teach at the Sorbonne, and devoted herself to continuing the work that they had begun together.', 'She received a second Nobel Prize, for Chemistry, in 1911.', \"The Curie's research was crucial in the development of x-rays in surgery.\", 'During World War One Curie helped to equip ambulances with x-ray equipment, which she herself drove to the front lines.', 'The International Red Cross made her head of its radiological service and she held training courses for medical orderlies and doctors in the new techniques.', 'Despite her success, Marie continued to face great opposition from male scientists in France, and she never received significant financial benefits from her work.', 'By the late 1920s her health was beginning to deteriorate.', 'She died on 4 July 1934 from leukaemia, caused by exposure to high-energy radiation from her research.', \"The Curies' eldest daughter Irene was herself a scientist and winner of the Nobel Prize for Chemistry.\"]\n"
     ]
    }
   ],
   "source": [
    "sentences = file_contents.split('\\n')\n",
    "        \n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 167)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_vector = count_vectorizer.fit_transform(sentences)\n",
    "\n",
    "transformed_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 146)\t1\n",
      "  (0, 72)\t1\n",
      "  (0, 128)\t1\n",
      "  (0, 56)\t1\n",
      "  (0, 96)\t1\n",
      "  (0, 144)\t1\n",
      "  (0, 101)\t2\n",
      "  (0, 103)\t1\n",
      "  (0, 27)\t1\n",
      "  (0, 11)\t2\n",
      "  (0, 108)\t1\n",
      "  (0, 21)\t1\n",
      "  (0, 111)\t1\n",
      "  (0, 153)\t1\n",
      "  (0, 34)\t1\n",
      "  (0, 91)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 159)\t1\n",
      "  (1, 147)\t1\n",
      "  (1, 102)\t1\n",
      "  (1, 154)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 77)\t2\n",
      "  (1, 114)\t1\n",
      "  :\t:\n",
      "  (17, 8)\t1\n",
      "  (17, 42)\t1\n",
      "  (17, 62)\t2\n",
      "  (17, 124)\t1\n",
      "  (17, 23)\t1\n",
      "  (17, 82)\t1\n",
      "  (17, 147)\t1\n",
      "  (17, 102)\t1\n",
      "  (17, 131)\t1\n",
      "  (17, 72)\t1\n",
      "  (18, 160)\t1\n",
      "  (18, 127)\t1\n",
      "  (18, 80)\t1\n",
      "  (18, 48)\t1\n",
      "  (18, 28)\t1\n",
      "  (18, 73)\t1\n",
      "  (18, 59)\t1\n",
      "  (18, 35)\t1\n",
      "  (18, 37)\t1\n",
      "  (18, 114)\t1\n",
      "  (18, 99)\t1\n",
      "  (18, 144)\t2\n",
      "  (18, 101)\t1\n",
      "  (18, 11)\t1\n",
      "  (18, 153)\t1\n"
     ]
    }
   ],
   "source": [
    "print(transformed_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'marie': 91, 'curie': 34, 'was': 153, 'polish': 111, 'born': 21, 'physicist': 108, 'and': 11, 'chemist': 27, 'one': 103, 'of': 101, 'the': 144, 'most': 96, 'famous': 56, 'scientists': 128, 'her': 72, 'time': 146, 'together': 148, 'with': 161, 'husband': 76, 'pierre': 110, 'she': 131, 'awarded': 15, 'nobel': 99, 'prize': 114, 'in': 77, '1903': 4, 'went': 154, 'on': 102, 'to': 147, 'win': 159, 'another': 13, '1911': 6, 'sklodowska': 134, 'warsaw': 152, 'november': 100, '1867': 0, 'daughter': 37, 'teacher': 140, '1891': 1, 'paris': 107, 'study': 136, 'physics': 109, 'mathematics': 93, 'at': 14, 'sorbonne': 135, 'where': 157, 'met': 95, 'professor': 115, 'school': 126, 'they': 145, 'were': 155, 'married': 92, '1895': 2, 'curies': 35, 'worked': 164, 'investigating': 79, 'radioactivity': 117, 'building': 22, 'work': 163, 'german': 64, 'roentgen': 125, 'french': 61, 'becquerel': 17, 'july': 82, '1898': 3, 'announced': 12, 'discovery': 43, 'new': 98, 'chemical': 26, 'element': 49, 'polonium': 112, 'end': 50, 'year': 166, 'radium': 119, 'along': 9, 'for': 59, 'life': 87, 'cut': 36, 'short': 132, '1906': 5, 'when': 156, 'he': 67, 'knocked': 84, 'down': 45, 'killed': 83, 'by': 23, 'carriage': 24, 'took': 149, 'over': 106, 'his': 75, 'teaching': 141, 'post': 113, 'becoming': 16, 'first': 58, 'woman': 162, 'teach': 139, 'devoted': 41, 'herself': 73, 'continuing': 30, 'that': 143, 'had': 66, 'begun': 19, 'received': 122, 'second': 129, 'chemistry': 28, 'research': 124, 'crucial': 33, 'development': 40, 'rays': 121, 'surgery': 138, 'during': 47, 'world': 165, 'war': 151, 'helped': 71, 'equip': 52, 'ambulances': 10, 'ray': 120, 'equipment': 53, 'which': 158, 'drove': 46, 'front': 63, 'lines': 88, 'international': 78, 'red': 123, 'cross': 32, 'made': 89, 'head': 68, 'its': 81, 'radiological': 118, 'service': 130, 'held': 70, 'training': 150, 'courses': 31, 'medical': 94, 'orderlies': 105, 'doctors': 44, 'techniques': 142, 'despite': 38, 'success': 137, 'continued': 29, 'face': 55, 'great': 65, 'opposition': 104, 'from': 62, 'male': 90, 'france': 60, 'never': 97, 'significant': 133, 'financial': 57, 'benefits': 20, 'late': 85, '1920s': 7, 'health': 69, 'beginning': 18, 'deteriorate': 39, 'died': 42, '1934': 8, 'leukaemia': 86, 'caused': 25, 'exposure': 54, 'high': 74, 'energy': 51, 'radiation': 116, 'eldest': 48, 'irene': 80, 'scientist': 127, 'winner': 160}\n"
     ]
    }
   ],
   "source": [
    "print(count_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We lost:\n",
    "\n",
    "* The meaning of text corpus\n",
    "* The ordering of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['time', 'her', 'scientists', 'famous', 'most', 'the', 'of', 'one',\n",
       "        'chemist', 'and', 'physicist', 'born', 'polish', 'was', 'curie',\n",
       "        'marie'], dtype='<U13'),\n",
       " array(['1911', 'another', 'win', 'to', 'on', 'went', '1903', 'in',\n",
       "        'prize', 'nobel', 'awarded', 'she', 'pierre', 'husband', 'with',\n",
       "        'together', 'her', 'the', 'and', 'was'], dtype='<U13'),\n",
       " array(['teacher', 'daughter', '1867', 'november', 'warsaw', 'sklodowska',\n",
       "        'on', 'in', 'the', 'of', 'born', 'was', 'marie'], dtype='<U13'),\n",
       " array(['school', 'professor', 'met', 'where', 'sorbonne', 'at',\n",
       "        'mathematics', 'physics', 'study', 'paris', '1891', 'to', 'went',\n",
       "        'in', 'she', 'pierre', 'the', 'of', 'and', 'curie'], dtype='<U13'),\n",
       " array(['1895', 'married', 'were', 'they', 'in'], dtype='<U13'),\n",
       " array(['becquerel', 'french', 'roentgen', 'german', 'work', 'building',\n",
       "        'radioactivity', 'investigating', 'worked', 'curies', 'on',\n",
       "        'together', 'the', 'of', 'and', 'physicist'], dtype='<U13'),\n",
       " array(['polonium', 'element', 'chemical', 'new', 'discovery', 'announced',\n",
       "        '1898', 'july', 'curies', 'in', 'the', 'of'], dtype='<U13'),\n",
       " array(['radium', 'year', 'end', 'discovery', 'announced', 'they', 'at',\n",
       "        'another', 'the', 'of'], dtype='<U13'),\n",
       " array(['for', 'along', 'becquerel', 'curies', 'were', 'physics', '1903',\n",
       "        'in', 'prize', 'nobel', 'awarded', 'with', 'the'], dtype='<U13'),\n",
       " array(['carriage', 'by', 'killed', 'down', 'knocked', 'he', 'when',\n",
       "        '1906', 'short', 'cut', 'life', 'in', 'pierre', 'and', 'was'],\n",
       "       dtype='<U13'),\n",
       " array(['begun', 'had', 'that', 'continuing', 'herself', 'devoted',\n",
       "        'teach', 'woman', 'first', 'becoming', 'post', 'teaching', 'his',\n",
       "        'over', 'took', 'work', 'they', 'sorbonne', 'at', 'to', 'together',\n",
       "        'the', 'and', 'marie'], dtype='<U13'),\n",
       " array(['chemistry', 'second', 'received', 'for', '1911', 'in', 'prize',\n",
       "        'nobel', 'she'], dtype='<U13'),\n",
       " array(['surgery', 'rays', 'development', 'crucial', 'research', 'in',\n",
       "        'the', 'of', 'was', 'curie'], dtype='<U13'),\n",
       " array(['lines', 'front', 'drove', 'which', 'equipment', 'ray',\n",
       "        'ambulances', 'equip', 'helped', 'war', 'world', 'during',\n",
       "        'herself', 'to', 'she', 'with', 'the', 'one', 'curie'],\n",
       "       dtype='<U13'),\n",
       " array(['techniques', 'doctors', 'orderlies', 'medical', 'courses',\n",
       "        'training', 'held', 'service', 'radiological', 'its', 'head',\n",
       "        'made', 'cross', 'red', 'international', 'for', 'new', 'in', 'she',\n",
       "        'her', 'the', 'of', 'and'], dtype='<U13'),\n",
       " array(['benefits', 'financial', 'significant', 'never', 'france', 'male',\n",
       "        'from', 'opposition', 'great', 'face', 'continued', 'success',\n",
       "        'despite', 'received', 'work', 'to', 'in', 'she', 'her',\n",
       "        'scientists', 'and', 'marie'], dtype='<U13'),\n",
       " array(['deteriorate', 'beginning', 'health', '1920s', 'late', 'by', 'to',\n",
       "        'her', 'the', 'was'], dtype='<U13'),\n",
       " array(['radiation', 'energy', 'high', 'exposure', 'caused', 'leukaemia',\n",
       "        '1934', 'died', 'from', 'research', 'by', 'july', 'to', 'on',\n",
       "        'she', 'her'], dtype='<U13'),\n",
       " array(['winner', 'scientist', 'irene', 'eldest', 'chemistry', 'herself',\n",
       "        'for', 'curies', 'daughter', 'prize', 'nobel', 'the', 'of', 'and',\n",
       "        'was'], dtype='<U13')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.inverse_transform(transformed_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><font color=\"green\"><h1>from file: 03-VectorizeTextAsABagOfNGrams_CountVectorizer_Nltk</h1></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize text as a bag-of-n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = [\"A bird in hand is worth two in the bush.\",\n",
    "              \"Good things come to those who wait.\",\n",
    "              \"These watches cost $1500! \",\n",
    "              \"There are other fish in the sea.\",\n",
    "              \"The ball is in your court.\",\n",
    "              \"Mr. Smith Goes to Washington \",\n",
    "              \"Doogie Howser M.D.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_vectorizer = CountVectorizer(ngram_range=(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_vector = n_gram_vectorizer.fit_transform(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bird in': 2,\n",
       " 'in hand': 10,\n",
       " 'hand is': 9,\n",
       " 'is worth': 14,\n",
       " 'worth two': 30,\n",
       " 'two in': 27,\n",
       " 'in the': 11,\n",
       " 'the bush': 19,\n",
       " 'good things': 8,\n",
       " 'things come': 23,\n",
       " 'come to': 3,\n",
       " 'to those': 25,\n",
       " 'those who': 24,\n",
       " 'who wait': 29,\n",
       " 'these watches': 22,\n",
       " 'watches cost': 28,\n",
       " 'cost 1500': 4,\n",
       " 'there are': 21,\n",
       " 'are other': 0,\n",
       " 'other fish': 16,\n",
       " 'fish in': 6,\n",
       " 'the sea': 20,\n",
       " 'the ball': 18,\n",
       " 'ball is': 1,\n",
       " 'is in': 13,\n",
       " 'in your': 12,\n",
       " 'your court': 31,\n",
       " 'mr smith': 15,\n",
       " 'smith goes': 17,\n",
       " 'goes to': 7,\n",
       " 'to washington': 26,\n",
       " 'doogie howser': 5}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 1, 1, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bird': 5,\n",
       " 'in': 24,\n",
       " 'hand': 21,\n",
       " 'is': 28,\n",
       " 'worth': 61,\n",
       " 'two': 53,\n",
       " 'the': 38,\n",
       " 'bush': 7,\n",
       " 'bird in': 6,\n",
       " 'in hand': 25,\n",
       " 'hand is': 22,\n",
       " 'is worth': 30,\n",
       " 'worth two': 62,\n",
       " 'two in': 54,\n",
       " 'in the': 26,\n",
       " 'the bush': 40,\n",
       " 'good': 19,\n",
       " 'things': 46,\n",
       " 'come': 8,\n",
       " 'to': 50,\n",
       " 'those': 48,\n",
       " 'who': 59,\n",
       " 'wait': 55,\n",
       " 'good things': 20,\n",
       " 'things come': 47,\n",
       " 'come to': 9,\n",
       " 'to those': 51,\n",
       " 'those who': 49,\n",
       " 'who wait': 60,\n",
       " 'these': 44,\n",
       " 'watches': 57,\n",
       " 'cost': 10,\n",
       " '1500': 0,\n",
       " 'these watches': 45,\n",
       " 'watches cost': 58,\n",
       " 'cost 1500': 11,\n",
       " 'there': 42,\n",
       " 'are': 1,\n",
       " 'other': 33,\n",
       " 'fish': 15,\n",
       " 'sea': 35,\n",
       " 'there are': 43,\n",
       " 'are other': 2,\n",
       " 'other fish': 34,\n",
       " 'fish in': 16,\n",
       " 'the sea': 41,\n",
       " 'ball': 3,\n",
       " 'your': 63,\n",
       " 'court': 12,\n",
       " 'the ball': 39,\n",
       " 'ball is': 4,\n",
       " 'is in': 29,\n",
       " 'in your': 27,\n",
       " 'your court': 64,\n",
       " 'mr': 31,\n",
       " 'smith': 36,\n",
       " 'goes': 17,\n",
       " 'washington': 56,\n",
       " 'mr smith': 32,\n",
       " 'smith goes': 37,\n",
       " 'goes to': 18,\n",
       " 'to washington': 52,\n",
       " 'doogie': 13,\n",
       " 'howser': 23,\n",
       " 'doogie howser': 14}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "transformed_vector = n_gram_vectorizer.fit_transform(train_text)\n",
    "\n",
    "n_gram_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 0, 2, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_vector.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram and Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marie Curie was a Polish-born physicist and chemist and one of the most famous scientists of her time.\n",
      "Together with her husband Pierre, she was awarded the Nobel Prize in 1903, and she went on to win another in 1911.\n",
      "Marie Sklodowska was born in Warsaw on 7 November 1867, the daughter of a teacher.\n",
      "In 1891, she went to Paris to study physics and mathematics at the Sorbonne where she met Pierre Curie, professor of the School of Physics.\n",
      "They were married in 1895.\n",
      "The Curies worked together investigating radioactivity, building on the work of the German physicist Roentgen and the French physicist Becquerel.\n",
      "In July 1898, the Curies announced the discovery of a new chemical element, polonium.\n",
      "At the end of the year, they announced the discovery of another, radium.\n",
      "The Curies, along with Becquerel, were awarded the Nobel Prize for Physics in 1903.\n",
      "Pierre's life was cut short in 1906 when he was knocked down and killed by a carriage.\n",
      "Marie took over his teaching post, becoming the first woman to teach at the Sorbonne, and devoted herself to continuing the work that they had begun together.\n",
      "She received a second Nobel Prize, for Chemistry, in 1911.\n",
      "The Curie's research was crucial in the development of x-rays in surgery.\n",
      "During World War One Curie helped to equip ambulances with x-ray equipment, which she herself drove to the front lines.\n",
      "The International Red Cross made her head of its radiological service and she held training courses for medical orderlies and doctors in the new techniques.\n",
      "Despite her success, Marie continued to face great opposition from male scientists in France, and she never received significant financial benefits from her work.\n",
      "By the late 1920s her health was beginning to deteriorate.\n",
      "She died on 4 July 1934 from leukaemia, caused by exposure to high-energy radiation from her research.\n",
      "The Curies' eldest daughter Irene was herself a scientist and winner of the Nobel Prize for Chemistry.\n"
     ]
    }
   ],
   "source": [
    "with open('./datasets/biography.txt', 'r') as f:\n",
    "    file_contents = f.read()\n",
    "\n",
    "print(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Marie Curie was a Polish-born physicist and chemist and one of the most famous scientists of her time.', 'Together with her husband Pierre, she was awarded the Nobel Prize in 1903, and she went on to win another in 1911.', 'Marie Sklodowska was born in Warsaw on 7 November 1867, the daughter of a teacher.', 'In 1891, she went to Paris to study physics and mathematics at the Sorbonne where she met Pierre Curie, professor of the School of Physics.', 'They were married in 1895.', 'The Curies worked together investigating radioactivity, building on the work of the German physicist Roentgen and the French physicist Becquerel.', 'In July 1898, the Curies announced the discovery of a new chemical element, polonium.', 'At the end of the year, they announced the discovery of another, radium.', 'The Curies, along with Becquerel, were awarded the Nobel Prize for Physics in 1903.', \"Pierre's life was cut short in 1906 when he was knocked down and killed by a carriage.\", 'Marie took over his teaching post, becoming the first woman to teach at the Sorbonne, and devoted herself to continuing the work that they had begun together.', 'She received a second Nobel Prize, for Chemistry, in 1911.', \"The Curie's research was crucial in the development of x-rays in surgery.\", 'During World War One Curie helped to equip ambulances with x-ray equipment, which she herself drove to the front lines.', 'The International Red Cross made her head of its radiological service and she held training courses for medical orderlies and doctors in the new techniques.', 'Despite her success, Marie continued to face great opposition from male scientists in France, and she never received significant financial benefits from her work.', 'By the late 1920s her health was beginning to deteriorate.', 'She died on 4 July 1934 from leukaemia, caused by exposure to high-energy radiation from her research.', \"The Curies' eldest daughter Irene was herself a scientist and winner of the Nobel Prize for Chemistry.\"]\n"
     ]
    }
   ],
   "source": [
    "sentences = file_contents.split('\\n')\n",
    "        \n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_vectorizer = CountVectorizer(ngram_range=(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_vector = n_gram_vectorizer.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'marie curie': 251,\n",
       " 'curie was': 92,\n",
       " 'was polish': 504,\n",
       " 'polish born': 330,\n",
       " 'born physicist': 59,\n",
       " 'physicist and': 315,\n",
       " 'and chemist': 18,\n",
       " 'chemist and': 72,\n",
       " 'and one': 28,\n",
       " 'one of': 305,\n",
       " 'of the': 289,\n",
       " 'the most': 437,\n",
       " 'most famous': 265,\n",
       " 'famous scientists': 142,\n",
       " 'scientists of': 367,\n",
       " 'of her': 279,\n",
       " 'her time': 191,\n",
       " 'marie curie was': 252,\n",
       " 'curie was polish': 93,\n",
       " 'was polish born': 505,\n",
       " 'polish born physicist': 331,\n",
       " 'born physicist and': 60,\n",
       " 'physicist and chemist': 316,\n",
       " 'and chemist and': 19,\n",
       " 'chemist and one': 73,\n",
       " 'and one of': 29,\n",
       " 'one of the': 306,\n",
       " 'of the most': 291,\n",
       " 'the most famous': 438,\n",
       " 'most famous scientists': 266,\n",
       " 'famous scientists of': 143,\n",
       " 'scientists of her': 368,\n",
       " 'of her time': 280,\n",
       " 'together with': 480,\n",
       " 'with her': 526,\n",
       " 'her husband': 186,\n",
       " 'husband pierre': 203,\n",
       " 'pierre she': 328,\n",
       " 'she was': 385,\n",
       " 'was awarded': 490,\n",
       " 'awarded the': 46,\n",
       " 'the nobel': 441,\n",
       " 'nobel prize': 272,\n",
       " 'prize in': 337,\n",
       " 'in 1903': 208,\n",
       " '1903 and': 6,\n",
       " 'and she': 30,\n",
       " 'she went': 387,\n",
       " 'went on': 506,\n",
       " 'on to': 301,\n",
       " 'to win': 476,\n",
       " 'win another': 520,\n",
       " 'another in': 40,\n",
       " 'in 1911': 212,\n",
       " 'together with her': 481,\n",
       " 'with her husband': 527,\n",
       " 'her husband pierre': 187,\n",
       " 'husband pierre she': 204,\n",
       " 'pierre she was': 329,\n",
       " 'she was awarded': 386,\n",
       " 'was awarded the': 491,\n",
       " 'awarded the nobel': 47,\n",
       " 'the nobel prize': 442,\n",
       " 'nobel prize in': 274,\n",
       " 'prize in 1903': 338,\n",
       " 'in 1903 and': 209,\n",
       " '1903 and she': 7,\n",
       " 'and she went': 33,\n",
       " 'she went on': 388,\n",
       " 'went on to': 507,\n",
       " 'on to win': 302,\n",
       " 'to win another': 477,\n",
       " 'win another in': 521,\n",
       " 'another in 1911': 41,\n",
       " 'marie sklodowska': 253,\n",
       " 'sklodowska was': 394,\n",
       " 'was born': 494,\n",
       " 'born in': 57,\n",
       " 'in warsaw': 221,\n",
       " 'warsaw on': 488,\n",
       " 'on november': 297,\n",
       " 'november 1867': 275,\n",
       " '1867 the': 0,\n",
       " 'the daughter': 417,\n",
       " 'daughter of': 106,\n",
       " 'of teacher': 288,\n",
       " 'marie sklodowska was': 254,\n",
       " 'sklodowska was born': 395,\n",
       " 'was born in': 495,\n",
       " 'born in warsaw': 58,\n",
       " 'in warsaw on': 222,\n",
       " 'warsaw on november': 489,\n",
       " 'on november 1867': 298,\n",
       " 'november 1867 the': 276,\n",
       " '1867 the daughter': 1,\n",
       " 'the daughter of': 418,\n",
       " 'daughter of teacher': 107,\n",
       " 'in 1891': 205,\n",
       " '1891 she': 2,\n",
       " 'went to': 508,\n",
       " 'to paris': 468,\n",
       " 'paris to': 313,\n",
       " 'to study': 470,\n",
       " 'study physics': 400,\n",
       " 'physics and': 320,\n",
       " 'and mathematics': 26,\n",
       " 'mathematics at': 259,\n",
       " 'at the': 43,\n",
       " 'the sorbonne': 445,\n",
       " 'sorbonne where': 398,\n",
       " 'where she': 516,\n",
       " 'she met': 379,\n",
       " 'met pierre': 263,\n",
       " 'pierre curie': 324,\n",
       " 'curie professor': 88,\n",
       " 'professor of': 339,\n",
       " 'the school': 443,\n",
       " 'school of': 361,\n",
       " 'of physics': 285,\n",
       " 'in 1891 she': 206,\n",
       " '1891 she went': 3,\n",
       " 'she went to': 389,\n",
       " 'went to paris': 509,\n",
       " 'to paris to': 469,\n",
       " 'paris to study': 314,\n",
       " 'to study physics': 471,\n",
       " 'study physics and': 401,\n",
       " 'physics and mathematics': 321,\n",
       " 'and mathematics at': 27,\n",
       " 'mathematics at the': 260,\n",
       " 'at the sorbonne': 45,\n",
       " 'the sorbonne where': 447,\n",
       " 'sorbonne where she': 399,\n",
       " 'where she met': 517,\n",
       " 'she met pierre': 380,\n",
       " 'met pierre curie': 264,\n",
       " 'pierre curie professor': 325,\n",
       " 'curie professor of': 89,\n",
       " 'professor of the': 340,\n",
       " 'of the school': 293,\n",
       " 'the school of': 444,\n",
       " 'school of physics': 362,\n",
       " 'they were': 457,\n",
       " 'were married': 512,\n",
       " 'married in': 257,\n",
       " 'in 1895': 207,\n",
       " 'they were married': 458,\n",
       " 'were married in': 513,\n",
       " 'married in 1895': 258,\n",
       " 'the curies': 412,\n",
       " 'curies worked': 100,\n",
       " 'worked together': 536,\n",
       " 'together investigating': 478,\n",
       " 'investigating radioactivity': 225,\n",
       " 'radioactivity building': 343,\n",
       " 'building on': 61,\n",
       " 'on the': 299,\n",
       " 'the work': 448,\n",
       " 'work of': 532,\n",
       " 'the german': 431,\n",
       " 'german physicist': 166,\n",
       " 'physicist roentgen': 318,\n",
       " 'roentgen and': 359,\n",
       " 'and the': 34,\n",
       " 'the french': 427,\n",
       " 'french physicist': 156,\n",
       " 'physicist becquerel': 317,\n",
       " 'the curies worked': 416,\n",
       " 'curies worked together': 101,\n",
       " 'worked together investigating': 537,\n",
       " 'together investigating radioactivity': 479,\n",
       " 'investigating radioactivity building': 226,\n",
       " 'radioactivity building on': 344,\n",
       " 'building on the': 62,\n",
       " 'on the work': 300,\n",
       " 'the work of': 449,\n",
       " 'work of the': 533,\n",
       " 'of the german': 290,\n",
       " 'the german physicist': 432,\n",
       " 'german physicist roentgen': 167,\n",
       " 'physicist roentgen and': 319,\n",
       " 'roentgen and the': 360,\n",
       " 'and the french': 35,\n",
       " 'the french physicist': 428,\n",
       " 'french physicist becquerel': 157,\n",
       " 'in july': 215,\n",
       " 'july 1898': 231,\n",
       " '1898 the': 4,\n",
       " 'curies announced': 96,\n",
       " 'announced the': 38,\n",
       " 'the discovery': 421,\n",
       " 'discovery of': 116,\n",
       " 'of new': 283,\n",
       " 'new chemical': 269,\n",
       " 'chemical element': 70,\n",
       " 'element polonium': 129,\n",
       " 'in july 1898': 216,\n",
       " 'july 1898 the': 232,\n",
       " '1898 the curies': 5,\n",
       " 'the curies announced': 414,\n",
       " 'curies announced the': 97,\n",
       " 'announced the discovery': 39,\n",
       " 'the discovery of': 422,\n",
       " 'discovery of new': 118,\n",
       " 'of new chemical': 284,\n",
       " 'new chemical element': 270,\n",
       " 'chemical element polonium': 71,\n",
       " 'the end': 423,\n",
       " 'end of': 130,\n",
       " 'the year': 451,\n",
       " 'year they': 540,\n",
       " 'they announced': 453,\n",
       " 'of another': 277,\n",
       " 'another radium': 42,\n",
       " 'at the end': 44,\n",
       " 'the end of': 424,\n",
       " 'end of the': 131,\n",
       " 'of the year': 294,\n",
       " 'the year they': 452,\n",
       " 'year they announced': 541,\n",
       " 'they announced the': 454,\n",
       " 'discovery of another': 117,\n",
       " 'of another radium': 278,\n",
       " 'curies along': 94,\n",
       " 'along with': 14,\n",
       " 'with becquerel': 524,\n",
       " 'becquerel were': 50,\n",
       " 'were awarded': 510,\n",
       " 'prize for': 334,\n",
       " 'for physics': 152,\n",
       " 'physics in': 322,\n",
       " 'the curies along': 413,\n",
       " 'curies along with': 95,\n",
       " 'along with becquerel': 15,\n",
       " 'with becquerel were': 525,\n",
       " 'becquerel were awarded': 51,\n",
       " 'were awarded the': 511,\n",
       " 'nobel prize for': 273,\n",
       " 'prize for physics': 336,\n",
       " 'for physics in': 153,\n",
       " 'physics in 1903': 323,\n",
       " 'pierre life': 326,\n",
       " 'life was': 243,\n",
       " 'was cut': 498,\n",
       " 'cut short': 102,\n",
       " 'short in': 390,\n",
       " 'in 1906': 210,\n",
       " '1906 when': 8,\n",
       " 'when he': 514,\n",
       " 'he was': 172,\n",
       " 'was knocked': 502,\n",
       " 'knocked down': 237,\n",
       " 'down and': 121,\n",
       " 'and killed': 24,\n",
       " 'killed by': 235,\n",
       " 'by carriage': 63,\n",
       " 'pierre life was': 327,\n",
       " 'life was cut': 244,\n",
       " 'was cut short': 499,\n",
       " 'cut short in': 103,\n",
       " 'short in 1906': 391,\n",
       " 'in 1906 when': 211,\n",
       " '1906 when he': 9,\n",
       " 'when he was': 515,\n",
       " 'he was knocked': 173,\n",
       " 'was knocked down': 503,\n",
       " 'knocked down and': 238,\n",
       " 'down and killed': 122,\n",
       " 'and killed by': 25,\n",
       " 'killed by carriage': 236,\n",
       " 'marie took': 255,\n",
       " 'took over': 482,\n",
       " 'over his': 311,\n",
       " 'his teaching': 201,\n",
       " 'teaching post': 406,\n",
       " 'post becoming': 332,\n",
       " 'becoming the': 48,\n",
       " 'the first': 425,\n",
       " 'first woman': 146,\n",
       " 'woman to': 530,\n",
       " 'to teach': 472,\n",
       " 'teach at': 404,\n",
       " 'sorbonne and': 396,\n",
       " 'and devoted': 20,\n",
       " 'devoted herself': 112,\n",
       " 'herself to': 197,\n",
       " 'to continuing': 459,\n",
       " 'continuing the': 78,\n",
       " 'work that': 534,\n",
       " 'that they': 408,\n",
       " 'they had': 455,\n",
       " 'had begun': 170,\n",
       " 'begun together': 54,\n",
       " 'marie took over': 256,\n",
       " 'took over his': 483,\n",
       " 'over his teaching': 312,\n",
       " 'his teaching post': 202,\n",
       " 'teaching post becoming': 407,\n",
       " 'post becoming the': 333,\n",
       " 'becoming the first': 49,\n",
       " 'the first woman': 426,\n",
       " 'first woman to': 147,\n",
       " 'woman to teach': 531,\n",
       " 'to teach at': 473,\n",
       " 'teach at the': 405,\n",
       " 'the sorbonne and': 446,\n",
       " 'sorbonne and devoted': 397,\n",
       " 'and devoted herself': 21,\n",
       " 'devoted herself to': 113,\n",
       " 'herself to continuing': 198,\n",
       " 'to continuing the': 460,\n",
       " 'continuing the work': 79,\n",
       " 'the work that': 450,\n",
       " 'work that they': 535,\n",
       " 'that they had': 409,\n",
       " 'they had begun': 456,\n",
       " 'had begun together': 171,\n",
       " 'she received': 383,\n",
       " 'received second': 351,\n",
       " 'second nobel': 369,\n",
       " 'for chemistry': 148,\n",
       " 'chemistry in': 74,\n",
       " 'she received second': 384,\n",
       " 'received second nobel': 352,\n",
       " 'second nobel prize': 370,\n",
       " 'prize for chemistry': 335,\n",
       " 'for chemistry in': 149,\n",
       " 'chemistry in 1911': 75,\n",
       " 'the curie': 410,\n",
       " 'curie research': 90,\n",
       " 'research was': 357,\n",
       " 'was crucial': 496,\n",
       " 'crucial in': 84,\n",
       " 'in the': 218,\n",
       " 'the development': 419,\n",
       " 'development of': 110,\n",
       " 'of rays': 286,\n",
       " 'rays in': 349,\n",
       " 'in surgery': 217,\n",
       " 'the curie research': 411,\n",
       " 'curie research was': 91,\n",
       " 'research was crucial': 358,\n",
       " 'was crucial in': 497,\n",
       " 'crucial in the': 85,\n",
       " 'in the development': 219,\n",
       " 'the development of': 420,\n",
       " 'development of rays': 111,\n",
       " 'of rays in': 287,\n",
       " 'rays in surgery': 350,\n",
       " 'during world': 125,\n",
       " 'world war': 538,\n",
       " 'war one': 486,\n",
       " 'one curie': 303,\n",
       " 'curie helped': 86,\n",
       " 'helped to': 180,\n",
       " 'to equip': 462,\n",
       " 'equip ambulances': 134,\n",
       " 'ambulances with': 16,\n",
       " 'with ray': 528,\n",
       " 'ray equipment': 347,\n",
       " 'equipment which': 136,\n",
       " 'which she': 518,\n",
       " 'she herself': 377,\n",
       " 'herself drove': 193,\n",
       " 'drove to': 123,\n",
       " 'to the': 474,\n",
       " 'the front': 429,\n",
       " 'front lines': 165,\n",
       " 'during world war': 126,\n",
       " 'world war one': 539,\n",
       " 'war one curie': 487,\n",
       " 'one curie helped': 304,\n",
       " 'curie helped to': 87,\n",
       " 'helped to equip': 181,\n",
       " 'to equip ambulances': 463,\n",
       " 'equip ambulances with': 135,\n",
       " 'ambulances with ray': 17,\n",
       " 'with ray equipment': 529,\n",
       " 'ray equipment which': 348,\n",
       " 'equipment which she': 137,\n",
       " 'which she herself': 519,\n",
       " 'she herself drove': 378,\n",
       " 'herself drove to': 194,\n",
       " 'drove to the': 124,\n",
       " 'to the front': 475,\n",
       " 'the front lines': 430,\n",
       " 'the international': 433,\n",
       " 'international red': 223,\n",
       " 'red cross': 355,\n",
       " 'cross made': 82,\n",
       " 'made her': 245,\n",
       " 'her head': 182,\n",
       " 'head of': 174,\n",
       " 'of its': 281,\n",
       " 'its radiological': 229,\n",
       " 'radiological service': 345,\n",
       " 'service and': 371,\n",
       " 'she held': 375,\n",
       " 'held training': 178,\n",
       " 'training courses': 484,\n",
       " 'courses for': 80,\n",
       " 'for medical': 150,\n",
       " 'medical orderlies': 261,\n",
       " 'orderlies and': 309,\n",
       " 'and doctors': 22,\n",
       " 'doctors in': 119,\n",
       " 'the new': 439,\n",
       " 'new techniques': 271,\n",
       " 'the international red': 434,\n",
       " 'international red cross': 224,\n",
       " 'red cross made': 356,\n",
       " 'cross made her': 83,\n",
       " 'made her head': 246,\n",
       " 'her head of': 183,\n",
       " 'head of its': 175,\n",
       " 'of its radiological': 282,\n",
       " 'its radiological service': 230,\n",
       " 'radiological service and': 346,\n",
       " 'service and she': 372,\n",
       " 'and she held': 31,\n",
       " 'she held training': 376,\n",
       " 'held training courses': 179,\n",
       " 'training courses for': 485,\n",
       " 'courses for medical': 81,\n",
       " 'for medical orderlies': 151,\n",
       " 'medical orderlies and': 262,\n",
       " 'orderlies and doctors': 310,\n",
       " 'and doctors in': 23,\n",
       " 'doctors in the': 120,\n",
       " 'in the new': 220,\n",
       " 'the new techniques': 440,\n",
       " 'despite her': 108,\n",
       " 'her success': 189,\n",
       " 'success marie': 402,\n",
       " 'marie continued': 249,\n",
       " 'continued to': 76,\n",
       " 'to face': 464,\n",
       " 'face great': 140,\n",
       " 'great opposition': 168,\n",
       " 'opposition from': 307,\n",
       " 'from male': 163,\n",
       " 'male scientists': 247,\n",
       " 'scientists in': 365,\n",
       " 'in france': 213,\n",
       " 'france and': 154,\n",
       " 'she never': 381,\n",
       " 'never received': 267,\n",
       " 'received significant': 353,\n",
       " 'significant financial': 392,\n",
       " 'financial benefits': 144,\n",
       " 'benefits from': 55,\n",
       " 'from her': 158,\n",
       " 'her work': 192,\n",
       " 'despite her success': 109,\n",
       " 'her success marie': 190,\n",
       " 'success marie continued': 403,\n",
       " 'marie continued to': 250,\n",
       " 'continued to face': 77,\n",
       " 'to face great': 465,\n",
       " 'face great opposition': 141,\n",
       " 'great opposition from': 169,\n",
       " 'opposition from male': 308,\n",
       " 'from male scientists': 164,\n",
       " 'male scientists in': 248,\n",
       " 'scientists in france': 366,\n",
       " 'in france and': 214,\n",
       " 'france and she': 155,\n",
       " 'and she never': 32,\n",
       " 'she never received': 382,\n",
       " 'never received significant': 268,\n",
       " 'received significant financial': 354,\n",
       " 'significant financial benefits': 393,\n",
       " 'financial benefits from': 145,\n",
       " 'benefits from her': 56,\n",
       " 'from her work': 160,\n",
       " 'by the': 66,\n",
       " 'the late': 435,\n",
       " 'late 1920s': 239,\n",
       " '1920s her': 10,\n",
       " 'her health': 184,\n",
       " 'health was': 176,\n",
       " 'was beginning': 492,\n",
       " 'beginning to': 52,\n",
       " 'to deteriorate': 461,\n",
       " 'by the late': 67,\n",
       " 'the late 1920s': 436,\n",
       " 'late 1920s her': 240,\n",
       " '1920s her health': 11,\n",
       " 'her health was': 185,\n",
       " 'health was beginning': 177,\n",
       " 'was beginning to': 493,\n",
       " 'beginning to deteriorate': 53,\n",
       " 'she died': 373,\n",
       " 'died on': 114,\n",
       " 'on july': 295,\n",
       " 'july 1934': 233,\n",
       " '1934 from': 12,\n",
       " 'from leukaemia': 161,\n",
       " 'leukaemia caused': 241,\n",
       " 'caused by': 68,\n",
       " 'by exposure': 64,\n",
       " 'exposure to': 138,\n",
       " 'to high': 466,\n",
       " 'high energy': 199,\n",
       " 'energy radiation': 132,\n",
       " 'radiation from': 341,\n",
       " 'her research': 188,\n",
       " 'she died on': 374,\n",
       " 'died on july': 115,\n",
       " 'on july 1934': 296,\n",
       " 'july 1934 from': 234,\n",
       " '1934 from leukaemia': 13,\n",
       " 'from leukaemia caused': 162,\n",
       " 'leukaemia caused by': 242,\n",
       " 'caused by exposure': 69,\n",
       " 'by exposure to': 65,\n",
       " 'exposure to high': 139,\n",
       " 'to high energy': 467,\n",
       " 'high energy radiation': 200,\n",
       " 'energy radiation from': 133,\n",
       " 'radiation from her': 342,\n",
       " 'from her research': 159,\n",
       " 'curies eldest': 98,\n",
       " 'eldest daughter': 127,\n",
       " 'daughter irene': 104,\n",
       " 'irene was': 227,\n",
       " 'was herself': 500,\n",
       " 'herself scientist': 195,\n",
       " 'scientist and': 363,\n",
       " 'and winner': 36,\n",
       " 'winner of': 522,\n",
       " 'the curies eldest': 415,\n",
       " 'curies eldest daughter': 99,\n",
       " 'eldest daughter irene': 128,\n",
       " 'daughter irene was': 105,\n",
       " 'irene was herself': 228,\n",
       " 'was herself scientist': 501,\n",
       " 'herself scientist and': 196,\n",
       " 'scientist and winner': 364,\n",
       " 'and winner of': 37,\n",
       " 'winner of the': 523,\n",
       " 'of the nobel': 292}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = n_gram_vectorizer.vocabulary_\n",
    "\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_vectorizer.vocabulary_.get('marie curie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_vectorizer.vocabulary_.get('of her')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://stackoverflow.com/questions/11763613/python-list-of-ngrams-with-frequencies\n",
    "* https://medium.com/@cristhianboujon/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_count is a vector that contains the sum of each word occurrence in all texts in the corpus. In other words, we are adding the elements for each column of vector matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_vector.toarray()[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 3,\n",
       "       1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 3, 3, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count = transformed_vector.toarray().sum(axis=0)\n",
    "\n",
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('marie curie', 251), ('curie was', 92), ('was polish', 504), ('polish born', 330), ('born physicist', 59), ('physicist and', 315), ('and chemist', 18), ('chemist and', 72), ('and one', 28), ('one of', 305), ('of the', 289), ('the most', 437), ('most famous', 265), ('famous scientists', 142), ('scientists of', 367), ('of her', 279), ('her time', 191), ('marie curie was', 252), ('curie was polish', 93), ('was polish born', 505), ('polish born physicist', 331), ('born physicist and', 60), ('physicist and chemist', 316), ('and chemist and', 19), ('chemist and one', 73), ('and one of', 29), ('one of the', 306), ('of the most', 291), ('the most famous', 438), ('most famous scientists', 266), ('famous scientists of', 143), ('scientists of her', 368), ('of her time', 280), ('together with', 480), ('with her', 526), ('her husband', 186), ('husband pierre', 203), ('pierre she', 328), ('she was', 385), ('was awarded', 490), ('awarded the', 46), ('the nobel', 441), ('nobel prize', 272), ('prize in', 337), ('in 1903', 208), ('1903 and', 6), ('and she', 30), ('she went', 387), ('went on', 506), ('on to', 301), ('to win', 476), ('win another', 520), ('another in', 40), ('in 1911', 212), ('together with her', 481), ('with her husband', 527), ('her husband pierre', 187), ('husband pierre she', 204), ('pierre she was', 329), ('she was awarded', 386), ('was awarded the', 491), ('awarded the nobel', 47), ('the nobel prize', 442), ('nobel prize in', 274), ('prize in 1903', 338), ('in 1903 and', 209), ('1903 and she', 7), ('and she went', 33), ('she went on', 388), ('went on to', 507), ('on to win', 302), ('to win another', 477), ('win another in', 521), ('another in 1911', 41), ('marie sklodowska', 253), ('sklodowska was', 394), ('was born', 494), ('born in', 57), ('in warsaw', 221), ('warsaw on', 488), ('on november', 297), ('november 1867', 275), ('1867 the', 0), ('the daughter', 417), ('daughter of', 106), ('of teacher', 288), ('marie sklodowska was', 254), ('sklodowska was born', 395), ('was born in', 495), ('born in warsaw', 58), ('in warsaw on', 222), ('warsaw on november', 489), ('on november 1867', 298), ('november 1867 the', 276), ('1867 the daughter', 1), ('the daughter of', 418), ('daughter of teacher', 107), ('in 1891', 205), ('1891 she', 2), ('went to', 508), ('to paris', 468), ('paris to', 313), ('to study', 470), ('study physics', 400), ('physics and', 320), ('and mathematics', 26), ('mathematics at', 259), ('at the', 43), ('the sorbonne', 445), ('sorbonne where', 398), ('where she', 516), ('she met', 379), ('met pierre', 263), ('pierre curie', 324), ('curie professor', 88), ('professor of', 339), ('the school', 443), ('school of', 361), ('of physics', 285), ('in 1891 she', 206), ('1891 she went', 3), ('she went to', 389), ('went to paris', 509), ('to paris to', 469), ('paris to study', 314), ('to study physics', 471), ('study physics and', 401), ('physics and mathematics', 321), ('and mathematics at', 27), ('mathematics at the', 260), ('at the sorbonne', 45), ('the sorbonne where', 447), ('sorbonne where she', 399), ('where she met', 517), ('she met pierre', 380), ('met pierre curie', 264), ('pierre curie professor', 325), ('curie professor of', 89), ('professor of the', 340), ('of the school', 293), ('the school of', 444), ('school of physics', 362), ('they were', 457), ('were married', 512), ('married in', 257), ('in 1895', 207), ('they were married', 458), ('were married in', 513), ('married in 1895', 258), ('the curies', 412), ('curies worked', 100), ('worked together', 536), ('together investigating', 478), ('investigating radioactivity', 225), ('radioactivity building', 343), ('building on', 61), ('on the', 299), ('the work', 448), ('work of', 532), ('the german', 431), ('german physicist', 166), ('physicist roentgen', 318), ('roentgen and', 359), ('and the', 34), ('the french', 427), ('french physicist', 156), ('physicist becquerel', 317), ('the curies worked', 416), ('curies worked together', 101), ('worked together investigating', 537), ('together investigating radioactivity', 479), ('investigating radioactivity building', 226), ('radioactivity building on', 344), ('building on the', 62), ('on the work', 300), ('the work of', 449), ('work of the', 533), ('of the german', 290), ('the german physicist', 432), ('german physicist roentgen', 167), ('physicist roentgen and', 319), ('roentgen and the', 360), ('and the french', 35), ('the french physicist', 428), ('french physicist becquerel', 157), ('in july', 215), ('july 1898', 231), ('1898 the', 4), ('curies announced', 96), ('announced the', 38), ('the discovery', 421), ('discovery of', 116), ('of new', 283), ('new chemical', 269), ('chemical element', 70), ('element polonium', 129), ('in july 1898', 216), ('july 1898 the', 232), ('1898 the curies', 5), ('the curies announced', 414), ('curies announced the', 97), ('announced the discovery', 39), ('the discovery of', 422), ('discovery of new', 118), ('of new chemical', 284), ('new chemical element', 270), ('chemical element polonium', 71), ('the end', 423), ('end of', 130), ('the year', 451), ('year they', 540), ('they announced', 453), ('of another', 277), ('another radium', 42), ('at the end', 44), ('the end of', 424), ('end of the', 131), ('of the year', 294), ('the year they', 452), ('year they announced', 541), ('they announced the', 454), ('discovery of another', 117), ('of another radium', 278), ('curies along', 94), ('along with', 14), ('with becquerel', 524), ('becquerel were', 50), ('were awarded', 510), ('prize for', 334), ('for physics', 152), ('physics in', 322), ('the curies along', 413), ('curies along with', 95), ('along with becquerel', 15), ('with becquerel were', 525), ('becquerel were awarded', 51), ('were awarded the', 511), ('nobel prize for', 273), ('prize for physics', 336), ('for physics in', 153), ('physics in 1903', 323), ('pierre life', 326), ('life was', 243), ('was cut', 498), ('cut short', 102), ('short in', 390), ('in 1906', 210), ('1906 when', 8), ('when he', 514), ('he was', 172), ('was knocked', 502), ('knocked down', 237), ('down and', 121), ('and killed', 24), ('killed by', 235), ('by carriage', 63), ('pierre life was', 327), ('life was cut', 244), ('was cut short', 499), ('cut short in', 103), ('short in 1906', 391), ('in 1906 when', 211), ('1906 when he', 9), ('when he was', 515), ('he was knocked', 173), ('was knocked down', 503), ('knocked down and', 238), ('down and killed', 122), ('and killed by', 25), ('killed by carriage', 236), ('marie took', 255), ('took over', 482), ('over his', 311), ('his teaching', 201), ('teaching post', 406), ('post becoming', 332), ('becoming the', 48), ('the first', 425), ('first woman', 146), ('woman to', 530), ('to teach', 472), ('teach at', 404), ('sorbonne and', 396), ('and devoted', 20), ('devoted herself', 112), ('herself to', 197), ('to continuing', 459), ('continuing the', 78), ('work that', 534), ('that they', 408), ('they had', 455), ('had begun', 170), ('begun together', 54), ('marie took over', 256), ('took over his', 483), ('over his teaching', 312), ('his teaching post', 202), ('teaching post becoming', 407), ('post becoming the', 333), ('becoming the first', 49), ('the first woman', 426), ('first woman to', 147), ('woman to teach', 531), ('to teach at', 473), ('teach at the', 405), ('the sorbonne and', 446), ('sorbonne and devoted', 397), ('and devoted herself', 21), ('devoted herself to', 113), ('herself to continuing', 198), ('to continuing the', 460), ('continuing the work', 79), ('the work that', 450), ('work that they', 535), ('that they had', 409), ('they had begun', 456), ('had begun together', 171), ('she received', 383), ('received second', 351), ('second nobel', 369), ('for chemistry', 148), ('chemistry in', 74), ('she received second', 384), ('received second nobel', 352), ('second nobel prize', 370), ('prize for chemistry', 335), ('for chemistry in', 149), ('chemistry in 1911', 75), ('the curie', 410), ('curie research', 90), ('research was', 357), ('was crucial', 496), ('crucial in', 84), ('in the', 218), ('the development', 419), ('development of', 110), ('of rays', 286), ('rays in', 349), ('in surgery', 217), ('the curie research', 411), ('curie research was', 91), ('research was crucial', 358), ('was crucial in', 497), ('crucial in the', 85), ('in the development', 219), ('the development of', 420), ('development of rays', 111), ('of rays in', 287), ('rays in surgery', 350), ('during world', 125), ('world war', 538), ('war one', 486), ('one curie', 303), ('curie helped', 86), ('helped to', 180), ('to equip', 462), ('equip ambulances', 134), ('ambulances with', 16), ('with ray', 528), ('ray equipment', 347), ('equipment which', 136), ('which she', 518), ('she herself', 377), ('herself drove', 193), ('drove to', 123), ('to the', 474), ('the front', 429), ('front lines', 165), ('during world war', 126), ('world war one', 539), ('war one curie', 487), ('one curie helped', 304), ('curie helped to', 87), ('helped to equip', 181), ('to equip ambulances', 463), ('equip ambulances with', 135), ('ambulances with ray', 17), ('with ray equipment', 529), ('ray equipment which', 348), ('equipment which she', 137), ('which she herself', 519), ('she herself drove', 378), ('herself drove to', 194), ('drove to the', 124), ('to the front', 475), ('the front lines', 430), ('the international', 433), ('international red', 223), ('red cross', 355), ('cross made', 82), ('made her', 245), ('her head', 182), ('head of', 174), ('of its', 281), ('its radiological', 229), ('radiological service', 345), ('service and', 371), ('she held', 375), ('held training', 178), ('training courses', 484), ('courses for', 80), ('for medical', 150), ('medical orderlies', 261), ('orderlies and', 309), ('and doctors', 22), ('doctors in', 119), ('the new', 439), ('new techniques', 271), ('the international red', 434), ('international red cross', 224), ('red cross made', 356), ('cross made her', 83), ('made her head', 246), ('her head of', 183), ('head of its', 175), ('of its radiological', 282), ('its radiological service', 230), ('radiological service and', 346), ('service and she', 372), ('and she held', 31), ('she held training', 376), ('held training courses', 179), ('training courses for', 485), ('courses for medical', 81), ('for medical orderlies', 151), ('medical orderlies and', 262), ('orderlies and doctors', 310), ('and doctors in', 23), ('doctors in the', 120), ('in the new', 220), ('the new techniques', 440), ('despite her', 108), ('her success', 189), ('success marie', 402), ('marie continued', 249), ('continued to', 76), ('to face', 464), ('face great', 140), ('great opposition', 168), ('opposition from', 307), ('from male', 163), ('male scientists', 247), ('scientists in', 365), ('in france', 213), ('france and', 154), ('she never', 381), ('never received', 267), ('received significant', 353), ('significant financial', 392), ('financial benefits', 144), ('benefits from', 55), ('from her', 158), ('her work', 192), ('despite her success', 109), ('her success marie', 190), ('success marie continued', 403), ('marie continued to', 250), ('continued to face', 77), ('to face great', 465), ('face great opposition', 141), ('great opposition from', 169), ('opposition from male', 308), ('from male scientists', 164), ('male scientists in', 248), ('scientists in france', 366), ('in france and', 214), ('france and she', 155), ('and she never', 32), ('she never received', 382), ('never received significant', 268), ('received significant financial', 354), ('significant financial benefits', 393), ('financial benefits from', 145), ('benefits from her', 56), ('from her work', 160), ('by the', 66), ('the late', 435), ('late 1920s', 239), ('1920s her', 10), ('her health', 184), ('health was', 176), ('was beginning', 492), ('beginning to', 52), ('to deteriorate', 461), ('by the late', 67), ('the late 1920s', 436), ('late 1920s her', 240), ('1920s her health', 11), ('her health was', 185), ('health was beginning', 177), ('was beginning to', 493), ('beginning to deteriorate', 53), ('she died', 373), ('died on', 114), ('on july', 295), ('july 1934', 233), ('1934 from', 12), ('from leukaemia', 161), ('leukaemia caused', 241), ('caused by', 68), ('by exposure', 64), ('exposure to', 138), ('to high', 466), ('high energy', 199), ('energy radiation', 132), ('radiation from', 341), ('her research', 188), ('she died on', 374), ('died on july', 115), ('on july 1934', 296), ('july 1934 from', 234), ('1934 from leukaemia', 13), ('from leukaemia caused', 162), ('leukaemia caused by', 242), ('caused by exposure', 69), ('by exposure to', 65), ('exposure to high', 139), ('to high energy', 467), ('high energy radiation', 200), ('energy radiation from', 133), ('radiation from her', 342), ('from her research', 159), ('curies eldest', 98), ('eldest daughter', 127), ('daughter irene', 104), ('irene was', 227), ('was herself', 500), ('herself scientist', 195), ('scientist and', 363), ('and winner', 36), ('winner of', 522), ('the curies eldest', 415), ('curies eldest daughter', 99), ('eldest daughter irene', 128), ('daughter irene was', 105), ('irene was herself', 228), ('was herself scientist', 501), ('herself scientist and', 196), ('scientist and winner', 364), ('and winner of', 37), ('winner of the', 523), ('of the nobel', 292)])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 'of the'),\n",
       " (4, 'the curies'),\n",
       " (4, 'nobel prize'),\n",
       " (3, 'the nobel prize'),\n",
       " (3, 'the nobel'),\n",
       " (3, 'prize for'),\n",
       " (3, 'nobel prize for'),\n",
       " (3, 'at the'),\n",
       " (3, 'and she'),\n",
       " (2, 'the work'),\n",
       " (2, 'the sorbonne'),\n",
       " (2, 'the discovery of'),\n",
       " (2, 'the discovery'),\n",
       " (2, 'she went'),\n",
       " (2, 'prize for chemistry'),\n",
       " (2, 'in the'),\n",
       " (2, 'in 1911'),\n",
       " (2, 'in 1903'),\n",
       " (2, 'from her'),\n",
       " (2, 'for chemistry'),\n",
       " (2, 'discovery of'),\n",
       " (2, 'awarded the nobel'),\n",
       " (2, 'awarded the'),\n",
       " (2, 'at the sorbonne'),\n",
       " (2, 'announced the discovery'),\n",
       " (2, 'announced the'),\n",
       " (1, 'year they announced'),\n",
       " (1, 'year they'),\n",
       " (1, 'world war one'),\n",
       " (1, 'world war'),\n",
       " (1, 'worked together investigating'),\n",
       " (1, 'worked together'),\n",
       " (1, 'work that they'),\n",
       " (1, 'work that'),\n",
       " (1, 'work of the'),\n",
       " (1, 'work of'),\n",
       " (1, 'woman to teach'),\n",
       " (1, 'woman to'),\n",
       " (1, 'with ray equipment'),\n",
       " (1, 'with ray'),\n",
       " (1, 'with her husband'),\n",
       " (1, 'with her'),\n",
       " (1, 'with becquerel were'),\n",
       " (1, 'with becquerel'),\n",
       " (1, 'winner of the'),\n",
       " (1, 'winner of'),\n",
       " (1, 'win another in'),\n",
       " (1, 'win another'),\n",
       " (1, 'which she herself'),\n",
       " (1, 'which she'),\n",
       " (1, 'where she met'),\n",
       " (1, 'where she'),\n",
       " (1, 'when he was'),\n",
       " (1, 'when he'),\n",
       " (1, 'were married in'),\n",
       " (1, 'were married'),\n",
       " (1, 'were awarded the'),\n",
       " (1, 'were awarded'),\n",
       " (1, 'went to paris'),\n",
       " (1, 'went to'),\n",
       " (1, 'went on to'),\n",
       " (1, 'went on'),\n",
       " (1, 'was polish born'),\n",
       " (1, 'was polish'),\n",
       " (1, 'was knocked down'),\n",
       " (1, 'was knocked'),\n",
       " (1, 'was herself scientist'),\n",
       " (1, 'was herself'),\n",
       " (1, 'was cut short'),\n",
       " (1, 'was cut'),\n",
       " (1, 'was crucial in'),\n",
       " (1, 'was crucial'),\n",
       " (1, 'was born in'),\n",
       " (1, 'was born'),\n",
       " (1, 'was beginning to'),\n",
       " (1, 'was beginning'),\n",
       " (1, 'was awarded the'),\n",
       " (1, 'was awarded'),\n",
       " (1, 'warsaw on november'),\n",
       " (1, 'warsaw on'),\n",
       " (1, 'war one curie'),\n",
       " (1, 'war one'),\n",
       " (1, 'training courses for'),\n",
       " (1, 'training courses'),\n",
       " (1, 'took over his'),\n",
       " (1, 'took over'),\n",
       " (1, 'together with her'),\n",
       " (1, 'together with'),\n",
       " (1, 'together investigating radioactivity'),\n",
       " (1, 'together investigating'),\n",
       " (1, 'to win another'),\n",
       " (1, 'to win'),\n",
       " (1, 'to the front'),\n",
       " (1, 'to the'),\n",
       " (1, 'to teach at'),\n",
       " (1, 'to teach'),\n",
       " (1, 'to study physics'),\n",
       " (1, 'to study'),\n",
       " (1, 'to paris to'),\n",
       " (1, 'to paris'),\n",
       " (1, 'to high energy'),\n",
       " (1, 'to high'),\n",
       " (1, 'to face great'),\n",
       " (1, 'to face'),\n",
       " (1, 'to equip ambulances'),\n",
       " (1, 'to equip'),\n",
       " (1, 'to deteriorate'),\n",
       " (1, 'to continuing the'),\n",
       " (1, 'to continuing'),\n",
       " (1, 'they were married'),\n",
       " (1, 'they were'),\n",
       " (1, 'they had begun'),\n",
       " (1, 'they had'),\n",
       " (1, 'they announced the'),\n",
       " (1, 'they announced'),\n",
       " (1, 'the year they'),\n",
       " (1, 'the year'),\n",
       " (1, 'the work that'),\n",
       " (1, 'the work of'),\n",
       " (1, 'the sorbonne where'),\n",
       " (1, 'the sorbonne and'),\n",
       " (1, 'the school of'),\n",
       " (1, 'the school'),\n",
       " (1, 'the new techniques'),\n",
       " (1, 'the new'),\n",
       " (1, 'the most famous'),\n",
       " (1, 'the most'),\n",
       " (1, 'the late 1920s'),\n",
       " (1, 'the late'),\n",
       " (1, 'the international red'),\n",
       " (1, 'the international'),\n",
       " (1, 'the german physicist'),\n",
       " (1, 'the german'),\n",
       " (1, 'the front lines'),\n",
       " (1, 'the front'),\n",
       " (1, 'the french physicist'),\n",
       " (1, 'the french'),\n",
       " (1, 'the first woman'),\n",
       " (1, 'the first'),\n",
       " (1, 'the end of'),\n",
       " (1, 'the end'),\n",
       " (1, 'the development of'),\n",
       " (1, 'the development'),\n",
       " (1, 'the daughter of'),\n",
       " (1, 'the daughter'),\n",
       " (1, 'the curies worked'),\n",
       " (1, 'the curies eldest'),\n",
       " (1, 'the curies announced'),\n",
       " (1, 'the curies along'),\n",
       " (1, 'the curie research'),\n",
       " (1, 'the curie'),\n",
       " (1, 'that they had'),\n",
       " (1, 'that they'),\n",
       " (1, 'teaching post becoming'),\n",
       " (1, 'teaching post'),\n",
       " (1, 'teach at the'),\n",
       " (1, 'teach at'),\n",
       " (1, 'success marie continued'),\n",
       " (1, 'success marie'),\n",
       " (1, 'study physics and'),\n",
       " (1, 'study physics'),\n",
       " (1, 'sorbonne where she'),\n",
       " (1, 'sorbonne where'),\n",
       " (1, 'sorbonne and devoted'),\n",
       " (1, 'sorbonne and'),\n",
       " (1, 'sklodowska was born'),\n",
       " (1, 'sklodowska was'),\n",
       " (1, 'significant financial benefits'),\n",
       " (1, 'significant financial'),\n",
       " (1, 'short in 1906'),\n",
       " (1, 'short in'),\n",
       " (1, 'she went to'),\n",
       " (1, 'she went on'),\n",
       " (1, 'she was awarded'),\n",
       " (1, 'she was'),\n",
       " (1, 'she received second'),\n",
       " (1, 'she received'),\n",
       " (1, 'she never received'),\n",
       " (1, 'she never'),\n",
       " (1, 'she met pierre'),\n",
       " (1, 'she met'),\n",
       " (1, 'she herself drove'),\n",
       " (1, 'she herself'),\n",
       " (1, 'she held training'),\n",
       " (1, 'she held'),\n",
       " (1, 'she died on'),\n",
       " (1, 'she died'),\n",
       " (1, 'service and she'),\n",
       " (1, 'service and'),\n",
       " (1, 'second nobel prize'),\n",
       " (1, 'second nobel'),\n",
       " (1, 'scientists of her'),\n",
       " (1, 'scientists of'),\n",
       " (1, 'scientists in france'),\n",
       " (1, 'scientists in'),\n",
       " (1, 'scientist and winner'),\n",
       " (1, 'scientist and'),\n",
       " (1, 'school of physics'),\n",
       " (1, 'school of'),\n",
       " (1, 'roentgen and the'),\n",
       " (1, 'roentgen and'),\n",
       " (1, 'research was crucial'),\n",
       " (1, 'research was'),\n",
       " (1, 'red cross made'),\n",
       " (1, 'red cross'),\n",
       " (1, 'received significant financial'),\n",
       " (1, 'received significant'),\n",
       " (1, 'received second nobel'),\n",
       " (1, 'received second'),\n",
       " (1, 'rays in surgery'),\n",
       " (1, 'rays in'),\n",
       " (1, 'ray equipment which'),\n",
       " (1, 'ray equipment'),\n",
       " (1, 'radiological service and'),\n",
       " (1, 'radiological service'),\n",
       " (1, 'radioactivity building on'),\n",
       " (1, 'radioactivity building'),\n",
       " (1, 'radiation from her'),\n",
       " (1, 'radiation from'),\n",
       " (1, 'professor of the'),\n",
       " (1, 'professor of'),\n",
       " (1, 'prize in 1903'),\n",
       " (1, 'prize in'),\n",
       " (1, 'prize for physics'),\n",
       " (1, 'post becoming the'),\n",
       " (1, 'post becoming'),\n",
       " (1, 'polish born physicist'),\n",
       " (1, 'polish born'),\n",
       " (1, 'pierre she was'),\n",
       " (1, 'pierre she'),\n",
       " (1, 'pierre life was'),\n",
       " (1, 'pierre life'),\n",
       " (1, 'pierre curie professor'),\n",
       " (1, 'pierre curie'),\n",
       " (1, 'physics in 1903'),\n",
       " (1, 'physics in'),\n",
       " (1, 'physics and mathematics'),\n",
       " (1, 'physics and'),\n",
       " (1, 'physicist roentgen and'),\n",
       " (1, 'physicist roentgen'),\n",
       " (1, 'physicist becquerel'),\n",
       " (1, 'physicist and chemist'),\n",
       " (1, 'physicist and'),\n",
       " (1, 'paris to study'),\n",
       " (1, 'paris to'),\n",
       " (1, 'over his teaching'),\n",
       " (1, 'over his'),\n",
       " (1, 'orderlies and doctors'),\n",
       " (1, 'orderlies and'),\n",
       " (1, 'opposition from male'),\n",
       " (1, 'opposition from'),\n",
       " (1, 'one of the'),\n",
       " (1, 'one of'),\n",
       " (1, 'one curie helped'),\n",
       " (1, 'one curie'),\n",
       " (1, 'on to win'),\n",
       " (1, 'on to'),\n",
       " (1, 'on the work'),\n",
       " (1, 'on the'),\n",
       " (1, 'on november 1867'),\n",
       " (1, 'on november'),\n",
       " (1, 'on july 1934'),\n",
       " (1, 'on july'),\n",
       " (1, 'of the year'),\n",
       " (1, 'of the school'),\n",
       " (1, 'of the nobel'),\n",
       " (1, 'of the most'),\n",
       " (1, 'of the german'),\n",
       " (1, 'of teacher'),\n",
       " (1, 'of rays in'),\n",
       " (1, 'of rays'),\n",
       " (1, 'of physics'),\n",
       " (1, 'of new chemical'),\n",
       " (1, 'of new'),\n",
       " (1, 'of its radiological'),\n",
       " (1, 'of its'),\n",
       " (1, 'of her time'),\n",
       " (1, 'of her'),\n",
       " (1, 'of another radium'),\n",
       " (1, 'of another'),\n",
       " (1, 'november 1867 the'),\n",
       " (1, 'november 1867'),\n",
       " (1, 'nobel prize in'),\n",
       " (1, 'new techniques'),\n",
       " (1, 'new chemical element'),\n",
       " (1, 'new chemical'),\n",
       " (1, 'never received significant'),\n",
       " (1, 'never received'),\n",
       " (1, 'most famous scientists'),\n",
       " (1, 'most famous'),\n",
       " (1, 'met pierre curie'),\n",
       " (1, 'met pierre'),\n",
       " (1, 'medical orderlies and'),\n",
       " (1, 'medical orderlies'),\n",
       " (1, 'mathematics at the'),\n",
       " (1, 'mathematics at'),\n",
       " (1, 'married in 1895'),\n",
       " (1, 'married in'),\n",
       " (1, 'marie took over'),\n",
       " (1, 'marie took'),\n",
       " (1, 'marie sklodowska was'),\n",
       " (1, 'marie sklodowska'),\n",
       " (1, 'marie curie was'),\n",
       " (1, 'marie curie'),\n",
       " (1, 'marie continued to'),\n",
       " (1, 'marie continued'),\n",
       " (1, 'male scientists in'),\n",
       " (1, 'male scientists'),\n",
       " (1, 'made her head'),\n",
       " (1, 'made her'),\n",
       " (1, 'life was cut'),\n",
       " (1, 'life was'),\n",
       " (1, 'leukaemia caused by'),\n",
       " (1, 'leukaemia caused'),\n",
       " (1, 'late 1920s her'),\n",
       " (1, 'late 1920s'),\n",
       " (1, 'knocked down and'),\n",
       " (1, 'knocked down'),\n",
       " (1, 'killed by carriage'),\n",
       " (1, 'killed by'),\n",
       " (1, 'july 1934 from'),\n",
       " (1, 'july 1934'),\n",
       " (1, 'july 1898 the'),\n",
       " (1, 'july 1898'),\n",
       " (1, 'its radiological service'),\n",
       " (1, 'its radiological'),\n",
       " (1, 'irene was herself'),\n",
       " (1, 'irene was'),\n",
       " (1, 'investigating radioactivity building'),\n",
       " (1, 'investigating radioactivity'),\n",
       " (1, 'international red cross'),\n",
       " (1, 'international red'),\n",
       " (1, 'in warsaw on'),\n",
       " (1, 'in warsaw'),\n",
       " (1, 'in the new'),\n",
       " (1, 'in the development'),\n",
       " (1, 'in surgery'),\n",
       " (1, 'in july 1898'),\n",
       " (1, 'in july'),\n",
       " (1, 'in france and'),\n",
       " (1, 'in france'),\n",
       " (1, 'in 1906 when'),\n",
       " (1, 'in 1906'),\n",
       " (1, 'in 1903 and'),\n",
       " (1, 'in 1895'),\n",
       " (1, 'in 1891 she'),\n",
       " (1, 'in 1891'),\n",
       " (1, 'husband pierre she'),\n",
       " (1, 'husband pierre'),\n",
       " (1, 'his teaching post'),\n",
       " (1, 'his teaching'),\n",
       " (1, 'high energy radiation'),\n",
       " (1, 'high energy'),\n",
       " (1, 'herself to continuing'),\n",
       " (1, 'herself to'),\n",
       " (1, 'herself scientist and'),\n",
       " (1, 'herself scientist'),\n",
       " (1, 'herself drove to'),\n",
       " (1, 'herself drove'),\n",
       " (1, 'her work'),\n",
       " (1, 'her time'),\n",
       " (1, 'her success marie'),\n",
       " (1, 'her success'),\n",
       " (1, 'her research'),\n",
       " (1, 'her husband pierre'),\n",
       " (1, 'her husband'),\n",
       " (1, 'her health was'),\n",
       " (1, 'her health'),\n",
       " (1, 'her head of'),\n",
       " (1, 'her head'),\n",
       " (1, 'helped to equip'),\n",
       " (1, 'helped to'),\n",
       " (1, 'held training courses'),\n",
       " (1, 'held training'),\n",
       " (1, 'health was beginning'),\n",
       " (1, 'health was'),\n",
       " (1, 'head of its'),\n",
       " (1, 'head of'),\n",
       " (1, 'he was knocked'),\n",
       " (1, 'he was'),\n",
       " (1, 'had begun together'),\n",
       " (1, 'had begun'),\n",
       " (1, 'great opposition from'),\n",
       " (1, 'great opposition'),\n",
       " (1, 'german physicist roentgen'),\n",
       " (1, 'german physicist'),\n",
       " (1, 'front lines'),\n",
       " (1, 'from male scientists'),\n",
       " (1, 'from male'),\n",
       " (1, 'from leukaemia caused'),\n",
       " (1, 'from leukaemia'),\n",
       " (1, 'from her work'),\n",
       " (1, 'from her research'),\n",
       " (1, 'french physicist becquerel'),\n",
       " (1, 'french physicist'),\n",
       " (1, 'france and she'),\n",
       " (1, 'france and'),\n",
       " (1, 'for physics in'),\n",
       " (1, 'for physics'),\n",
       " (1, 'for medical orderlies'),\n",
       " (1, 'for medical'),\n",
       " (1, 'for chemistry in'),\n",
       " (1, 'first woman to'),\n",
       " (1, 'first woman'),\n",
       " (1, 'financial benefits from'),\n",
       " (1, 'financial benefits'),\n",
       " (1, 'famous scientists of'),\n",
       " (1, 'famous scientists'),\n",
       " (1, 'face great opposition'),\n",
       " (1, 'face great'),\n",
       " (1, 'exposure to high'),\n",
       " (1, 'exposure to'),\n",
       " (1, 'equipment which she'),\n",
       " (1, 'equipment which'),\n",
       " (1, 'equip ambulances with'),\n",
       " (1, 'equip ambulances'),\n",
       " (1, 'energy radiation from'),\n",
       " (1, 'energy radiation'),\n",
       " (1, 'end of the'),\n",
       " (1, 'end of'),\n",
       " (1, 'element polonium'),\n",
       " (1, 'eldest daughter irene'),\n",
       " (1, 'eldest daughter'),\n",
       " (1, 'during world war'),\n",
       " (1, 'during world'),\n",
       " (1, 'drove to the'),\n",
       " (1, 'drove to'),\n",
       " (1, 'down and killed'),\n",
       " (1, 'down and'),\n",
       " (1, 'doctors in the'),\n",
       " (1, 'doctors in'),\n",
       " (1, 'discovery of new'),\n",
       " (1, 'discovery of another'),\n",
       " (1, 'died on july'),\n",
       " (1, 'died on'),\n",
       " (1, 'devoted herself to'),\n",
       " (1, 'devoted herself'),\n",
       " (1, 'development of rays'),\n",
       " (1, 'development of'),\n",
       " (1, 'despite her success'),\n",
       " (1, 'despite her'),\n",
       " (1, 'daughter of teacher'),\n",
       " (1, 'daughter of'),\n",
       " (1, 'daughter irene was'),\n",
       " (1, 'daughter irene'),\n",
       " (1, 'cut short in'),\n",
       " (1, 'cut short'),\n",
       " (1, 'curies worked together'),\n",
       " (1, 'curies worked'),\n",
       " (1, 'curies eldest daughter'),\n",
       " (1, 'curies eldest'),\n",
       " (1, 'curies announced the'),\n",
       " (1, 'curies announced'),\n",
       " (1, 'curies along with'),\n",
       " (1, 'curies along'),\n",
       " (1, 'curie was polish'),\n",
       " (1, 'curie was'),\n",
       " (1, 'curie research was'),\n",
       " (1, 'curie research'),\n",
       " (1, 'curie professor of'),\n",
       " (1, 'curie professor'),\n",
       " (1, 'curie helped to'),\n",
       " (1, 'curie helped'),\n",
       " (1, 'crucial in the'),\n",
       " (1, 'crucial in'),\n",
       " (1, 'cross made her'),\n",
       " (1, 'cross made'),\n",
       " (1, 'courses for medical'),\n",
       " (1, 'courses for'),\n",
       " (1, 'continuing the work'),\n",
       " (1, 'continuing the'),\n",
       " (1, 'continued to face'),\n",
       " (1, 'continued to'),\n",
       " (1, 'chemistry in 1911'),\n",
       " (1, 'chemistry in'),\n",
       " (1, 'chemist and one'),\n",
       " (1, 'chemist and'),\n",
       " (1, 'chemical element polonium'),\n",
       " (1, 'chemical element'),\n",
       " (1, 'caused by exposure'),\n",
       " (1, 'caused by'),\n",
       " (1, 'by the late'),\n",
       " (1, 'by the'),\n",
       " (1, 'by exposure to'),\n",
       " (1, 'by exposure'),\n",
       " (1, 'by carriage'),\n",
       " (1, 'building on the'),\n",
       " (1, 'building on'),\n",
       " (1, 'born physicist and'),\n",
       " (1, 'born physicist'),\n",
       " (1, 'born in warsaw'),\n",
       " (1, 'born in'),\n",
       " (1, 'benefits from her'),\n",
       " (1, 'benefits from'),\n",
       " (1, 'begun together'),\n",
       " (1, 'beginning to deteriorate'),\n",
       " (1, 'beginning to'),\n",
       " (1, 'becquerel were awarded'),\n",
       " (1, 'becquerel were'),\n",
       " (1, 'becoming the first'),\n",
       " (1, 'becoming the'),\n",
       " (1, 'at the end'),\n",
       " (1, 'another radium'),\n",
       " (1, 'another in 1911'),\n",
       " (1, 'another in'),\n",
       " (1, 'and winner of'),\n",
       " (1, 'and winner'),\n",
       " (1, 'and the french'),\n",
       " (1, 'and the'),\n",
       " (1, 'and she went'),\n",
       " (1, 'and she never'),\n",
       " (1, 'and she held'),\n",
       " (1, 'and one of'),\n",
       " (1, 'and one'),\n",
       " (1, 'and mathematics at'),\n",
       " (1, 'and mathematics'),\n",
       " (1, 'and killed by'),\n",
       " (1, 'and killed'),\n",
       " (1, 'and doctors in'),\n",
       " (1, 'and doctors'),\n",
       " (1, 'and devoted herself'),\n",
       " (1, 'and devoted'),\n",
       " (1, 'and chemist and'),\n",
       " (1, 'and chemist'),\n",
       " (1, 'ambulances with ray'),\n",
       " (1, 'ambulances with'),\n",
       " (1, 'along with becquerel'),\n",
       " (1, 'along with'),\n",
       " (1, '1934 from leukaemia'),\n",
       " (1, '1934 from'),\n",
       " (1, '1920s her health'),\n",
       " (1, '1920s her'),\n",
       " (1, '1906 when he'),\n",
       " (1, '1906 when'),\n",
       " (1, '1903 and she'),\n",
       " (1, '1903 and'),\n",
       " (1, '1898 the curies'),\n",
       " (1, '1898 the'),\n",
       " (1, '1891 she went'),\n",
       " (1, '1891 she'),\n",
       " (1, '1867 the daughter'),\n",
       " (1, '1867 the')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_word_list = sorted([(word_count[i], n_gram) for n_gram, i in vocabulary.items()], reverse=True)\n",
    "\n",
    "sorted_word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using nltk to find ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A bird in hand is worth two in the bush.',\n",
       " 'Good things come to those who wait.',\n",
       " 'These watches cost $1500! ',\n",
       " 'There are other fish in the sea.',\n",
       " 'The ball is in your court.',\n",
       " 'Mr. Smith Goes to Washington ',\n",
       " 'Doogie Howser M.D.']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "from nltk import trigrams\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'bird',\n",
       " 'in',\n",
       " 'hand',\n",
       " 'is',\n",
       " 'worth',\n",
       " 'two',\n",
       " 'in',\n",
       " 'the',\n",
       " 'bush',\n",
       " '.',\n",
       " 'Good',\n",
       " 'things',\n",
       " 'come',\n",
       " 'to',\n",
       " 'those',\n",
       " 'who',\n",
       " 'wait',\n",
       " '.',\n",
       " 'These',\n",
       " 'watches',\n",
       " 'cost',\n",
       " '$',\n",
       " '1500',\n",
       " '!',\n",
       " 'There',\n",
       " 'are',\n",
       " 'other',\n",
       " 'fish',\n",
       " 'in',\n",
       " 'the',\n",
       " 'sea',\n",
       " '.',\n",
       " 'The',\n",
       " 'ball',\n",
       " 'is',\n",
       " 'in',\n",
       " 'your',\n",
       " 'court',\n",
       " '.',\n",
       " 'Mr.',\n",
       " 'Smith',\n",
       " 'Goes',\n",
       " 'to',\n",
       " 'Washington',\n",
       " 'Doogie',\n",
       " 'Howser',\n",
       " 'M.D',\n",
       " '.']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens = word_tokenize(\" \".join(train_text))\n",
    "\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'bird'),\n",
       " ('bird', 'in'),\n",
       " ('in', 'hand'),\n",
       " ('hand', 'is'),\n",
       " ('is', 'worth'),\n",
       " ('worth', 'two'),\n",
       " ('two', 'in'),\n",
       " ('in', 'the'),\n",
       " ('the', 'bush'),\n",
       " ('bush', '.'),\n",
       " ('.', 'Good'),\n",
       " ('Good', 'things'),\n",
       " ('things', 'come'),\n",
       " ('come', 'to'),\n",
       " ('to', 'those'),\n",
       " ('those', 'who'),\n",
       " ('who', 'wait'),\n",
       " ('wait', '.'),\n",
       " ('.', 'These'),\n",
       " ('These', 'watches'),\n",
       " ('watches', 'cost'),\n",
       " ('cost', '$'),\n",
       " ('$', '1500'),\n",
       " ('1500', '!'),\n",
       " ('!', 'There'),\n",
       " ('There', 'are'),\n",
       " ('are', 'other'),\n",
       " ('other', 'fish'),\n",
       " ('fish', 'in'),\n",
       " ('in', 'the'),\n",
       " ('the', 'sea'),\n",
       " ('sea', '.'),\n",
       " ('.', 'The'),\n",
       " ('The', 'ball'),\n",
       " ('ball', 'is'),\n",
       " ('is', 'in'),\n",
       " ('in', 'your'),\n",
       " ('your', 'court'),\n",
       " ('court', '.'),\n",
       " ('.', 'Mr.'),\n",
       " ('Mr.', 'Smith'),\n",
       " ('Smith', 'Goes'),\n",
       " ('Goes', 'to'),\n",
       " ('to', 'Washington'),\n",
       " ('Washington', 'Doogie'),\n",
       " ('Doogie', 'Howser'),\n",
       " ('Howser', 'M.D'),\n",
       " ('M.D', '.')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_bigrams = bigrams(word_tokens)\n",
    "\n",
    "list(nltk_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'bird', 'in'),\n",
       " ('bird', 'in', 'hand'),\n",
       " ('in', 'hand', 'is'),\n",
       " ('hand', 'is', 'worth'),\n",
       " ('is', 'worth', 'two'),\n",
       " ('worth', 'two', 'in'),\n",
       " ('two', 'in', 'the'),\n",
       " ('in', 'the', 'bush'),\n",
       " ('the', 'bush', '.'),\n",
       " ('bush', '.', 'Good'),\n",
       " ('.', 'Good', 'things'),\n",
       " ('Good', 'things', 'come'),\n",
       " ('things', 'come', 'to'),\n",
       " ('come', 'to', 'those'),\n",
       " ('to', 'those', 'who'),\n",
       " ('those', 'who', 'wait'),\n",
       " ('who', 'wait', '.'),\n",
       " ('wait', '.', 'These'),\n",
       " ('.', 'These', 'watches'),\n",
       " ('These', 'watches', 'cost'),\n",
       " ('watches', 'cost', '$'),\n",
       " ('cost', '$', '1500'),\n",
       " ('$', '1500', '!'),\n",
       " ('1500', '!', 'There'),\n",
       " ('!', 'There', 'are'),\n",
       " ('There', 'are', 'other'),\n",
       " ('are', 'other', 'fish'),\n",
       " ('other', 'fish', 'in'),\n",
       " ('fish', 'in', 'the'),\n",
       " ('in', 'the', 'sea'),\n",
       " ('the', 'sea', '.'),\n",
       " ('sea', '.', 'The'),\n",
       " ('.', 'The', 'ball'),\n",
       " ('The', 'ball', 'is'),\n",
       " ('ball', 'is', 'in'),\n",
       " ('is', 'in', 'your'),\n",
       " ('in', 'your', 'court'),\n",
       " ('your', 'court', '.'),\n",
       " ('court', '.', 'Mr.'),\n",
       " ('.', 'Mr.', 'Smith'),\n",
       " ('Mr.', 'Smith', 'Goes'),\n",
       " ('Smith', 'Goes', 'to'),\n",
       " ('Goes', 'to', 'Washington'),\n",
       " ('to', 'Washington', 'Doogie'),\n",
       " ('Washington', 'Doogie', 'Howser'),\n",
       " ('Doogie', 'Howser', 'M.D'),\n",
       " ('Howser', 'M.D', '.')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_trigrams = trigrams(word_tokens)\n",
    "\n",
    "list(nltk_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A', 'bird', 'in', 'hand', 'is')\n",
      "('bird', 'in', 'hand', 'is', 'worth')\n",
      "('in', 'hand', 'is', 'worth', 'two')\n",
      "('hand', 'is', 'worth', 'two', 'in')\n",
      "('is', 'worth', 'two', 'in', 'the')\n",
      "('worth', 'two', 'in', 'the', 'bush')\n",
      "('two', 'in', 'the', 'bush', '.')\n",
      "('in', 'the', 'bush', '.', 'Good')\n",
      "('the', 'bush', '.', 'Good', 'things')\n",
      "('bush', '.', 'Good', 'things', 'come')\n",
      "('.', 'Good', 'things', 'come', 'to')\n",
      "('Good', 'things', 'come', 'to', 'those')\n",
      "('things', 'come', 'to', 'those', 'who')\n",
      "('come', 'to', 'those', 'who', 'wait')\n",
      "('to', 'those', 'who', 'wait', '.')\n",
      "('those', 'who', 'wait', '.', 'These')\n",
      "('who', 'wait', '.', 'These', 'watches')\n",
      "('wait', '.', 'These', 'watches', 'cost')\n",
      "('.', 'These', 'watches', 'cost', '$')\n",
      "('These', 'watches', 'cost', '$', '1500')\n",
      "('watches', 'cost', '$', '1500', '!')\n",
      "('cost', '$', '1500', '!', 'There')\n",
      "('$', '1500', '!', 'There', 'are')\n",
      "('1500', '!', 'There', 'are', 'other')\n",
      "('!', 'There', 'are', 'other', 'fish')\n",
      "('There', 'are', 'other', 'fish', 'in')\n",
      "('are', 'other', 'fish', 'in', 'the')\n",
      "('other', 'fish', 'in', 'the', 'sea')\n",
      "('fish', 'in', 'the', 'sea', '.')\n",
      "('in', 'the', 'sea', '.', 'The')\n",
      "('the', 'sea', '.', 'The', 'ball')\n",
      "('sea', '.', 'The', 'ball', 'is')\n",
      "('.', 'The', 'ball', 'is', 'in')\n",
      "('The', 'ball', 'is', 'in', 'your')\n",
      "('ball', 'is', 'in', 'your', 'court')\n",
      "('is', 'in', 'your', 'court', '.')\n",
      "('in', 'your', 'court', '.', 'Mr.')\n",
      "('your', 'court', '.', 'Mr.', 'Smith')\n",
      "('court', '.', 'Mr.', 'Smith', 'Goes')\n",
      "('.', 'Mr.', 'Smith', 'Goes', 'to')\n",
      "('Mr.', 'Smith', 'Goes', 'to', 'Washington')\n",
      "('Smith', 'Goes', 'to', 'Washington', 'Doogie')\n",
      "('Goes', 'to', 'Washington', 'Doogie', 'Howser')\n",
      "('to', 'Washington', 'Doogie', 'Howser', 'M.D')\n",
      "('Washington', 'Doogie', 'Howser', 'M.D', '.')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "fivegrams = ngrams(word_tokens, 5)\n",
    "\n",
    "for grams in fivegrams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://tedboy.github.io/nlps/generated/generated/nltk.BigramAssocMeasures\n",
    "* http://www.nltk.org/_modules/nltk/collocations.html#BigramCollocationFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramAssocMeasures\n",
    "from nltk.collocations import BigramCollocationFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = word_tokenize(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 'The'),\n",
       " ('of', 'the'),\n",
       " ('Nobel', 'Prize'),\n",
       " (',', 'and'),\n",
       " ('The', 'Curies'),\n",
       " ('and', 'she'),\n",
       " ('the', 'Nobel')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_measures = BigramAssocMeasures()\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(word_tokens)\n",
    "\n",
    "finder.apply_freq_filter(3)\n",
    "\n",
    "matches = finder.nbest(bigram_measures.raw_freq, 15)\n",
    "\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 'The'),\n",
       " ('of', 'the'),\n",
       " ('Nobel', 'Prize'),\n",
       " (',', 'and'),\n",
       " ('The', 'Curies'),\n",
       " ('and', 'she'),\n",
       " ('the', 'Nobel'),\n",
       " (',', 'she'),\n",
       " (',', 'the'),\n",
       " ('.', 'In'),\n",
       " ('.', 'Marie'),\n",
       " ('.', 'She'),\n",
       " ('1911', '.'),\n",
       " ('Prize', 'for'),\n",
       " ('announced', 'the')]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_measures = BigramAssocMeasures()\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(word_tokens)\n",
    "\n",
    "finder.apply_freq_filter(2)\n",
    "\n",
    "matches = finder.nbest(bigram_measures.raw_freq, 15)\n",
    "\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><font color=\"green\"><h1>from file: 04-VectorizeText_TfidfTransformer_TfidfVectorizer</h1></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>TfidfTransformer</b> :\n",
    "\n",
    "* Transform a count matrix to a normalized tf or tf-idf representation\n",
    "* Tf means term-frequency while tf-idf means term-frequency times inverse document-frequency. This is a common term weighting scheme in information retrieval, that has also found good use in document classification.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A <b>Term Frequency</b> is a count of how many times a word occurs in a given document (synonymous with bag of words).\n",
    "* The <b>Inverse Document Frequency</b> is the the number of times a word occurs in a corpus of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to create our training and testing document set and computing the term frequency matrix\n",
    "\n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a count vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train_text = [\"A bird in hand is worth two in the bush.\",\n",
    "              \"Good things come to those who wait.\",\n",
    "              \"These watches cost $1500! \",\n",
    "              \"There are other fish in the sea.\",\n",
    "              \"The ball is in your court.\",\n",
    "              \"Mr. Smith Goes to Washington \",\n",
    "              \"Doogie Howser M.D.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "frequency_term_matrix = count_vectorizer.fit_transform(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 33)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_term_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_term_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the tf-idf matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 33)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vector1 = tfidf_transformer.fit_transform(frequency_term_matrix)\n",
    "\n",
    "tfidf_vector1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.34908308, 0.34908308,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.34908308, 0.        , 0.49536976,\n",
       "        0.28976893, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.24768488, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.34908308, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.34908308, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.38665001, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.38665001, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.38665001, 0.38665001,\n",
       "        0.32095271, 0.        , 0.38665001, 0.        , 0.        ,\n",
       "        0.38665001, 0.        , 0.        ],\n",
       "       [0.5       , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.5       , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.5       , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.5       ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.40801493, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.40801493,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.28949879,\n",
       "        0.        , 0.        , 0.40801493, 0.40801493, 0.        ,\n",
       "        0.28949879, 0.40801493, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.46146654, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.46146654, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.3274243 ,\n",
       "        0.38305686, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.3274243 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.46146654],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.46180424, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.46180424, 0.        , 0.        , 0.46180424,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.38333718, 0.        , 0.        , 0.46180424, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.70710678, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.70710678, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vector1.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.34908308 0.34908308 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.34908308 0.         0.49536976 0.28976893 0.         0.\n",
      "  0.         0.         0.24768488 0.         0.         0.\n",
      "  0.         0.         0.34908308 0.         0.         0.\n",
      "  0.         0.34908308 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.38665001\n",
      "  0.         0.         0.         0.         0.         0.38665001\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.38665001\n",
      "  0.38665001 0.32095271 0.         0.38665001 0.         0.\n",
      "  0.38665001 0.         0.        ]\n",
      " [0.5        0.         0.         0.         0.         0.\n",
      "  0.5        0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.5        0.\n",
      "  0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.40801493 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.40801493 0.         0.\n",
      "  0.         0.         0.28949879 0.         0.         0.40801493\n",
      "  0.40801493 0.         0.28949879 0.40801493 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.46146654 0.         0.         0.\n",
      "  0.         0.46146654 0.         0.         0.         0.\n",
      "  0.         0.         0.3274243  0.38305686 0.         0.\n",
      "  0.         0.         0.3274243  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.46146654]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.46180424 0.\n",
      "  0.         0.         0.         0.         0.46180424 0.\n",
      "  0.         0.46180424 0.         0.         0.         0.\n",
      "  0.         0.38333718 0.         0.         0.46180424 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.70710678 0.         0.         0.\n",
      "  0.         0.70710678 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vector1.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer = CountVectorizer + TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bird': 3,\n",
       " 'in': 14,\n",
       " 'hand': 12,\n",
       " 'is': 15,\n",
       " 'worth': 31,\n",
       " 'two': 26,\n",
       " 'the': 20,\n",
       " 'bush': 4,\n",
       " 'good': 11,\n",
       " 'things': 23,\n",
       " 'come': 5,\n",
       " 'to': 25,\n",
       " 'those': 24,\n",
       " 'who': 30,\n",
       " 'wait': 27,\n",
       " 'these': 22,\n",
       " 'watches': 29,\n",
       " 'cost': 6,\n",
       " '1500': 0,\n",
       " 'there': 21,\n",
       " 'are': 1,\n",
       " 'other': 17,\n",
       " 'fish': 9,\n",
       " 'sea': 18,\n",
       " 'ball': 2,\n",
       " 'your': 32,\n",
       " 'court': 7,\n",
       " 'mr': 16,\n",
       " 'smith': 19,\n",
       " 'goes': 10,\n",
       " 'washington': 28,\n",
       " 'doogie': 8,\n",
       " 'howser': 13}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vector2 = tfidf_vectorizer.fit_transform(train_text)\n",
    "\n",
    "tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 33)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vector2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.38629436, 2.38629436, 2.38629436, 2.38629436, 2.38629436,\n",
       "       2.38629436, 2.38629436, 2.38629436, 2.38629436, 2.38629436,\n",
       "       2.38629436, 2.38629436, 2.38629436, 2.38629436, 1.69314718,\n",
       "       1.98082925, 2.38629436, 2.38629436, 2.38629436, 2.38629436,\n",
       "       1.69314718, 2.38629436, 2.38629436, 2.38629436, 2.38629436,\n",
       "       1.98082925, 2.38629436, 2.38629436, 2.38629436, 2.38629436,\n",
       "       2.38629436, 2.38629436, 2.38629436])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1500': 2.386294361119891,\n",
       " 'are': 2.386294361119891,\n",
       " 'ball': 2.386294361119891,\n",
       " 'bird': 2.386294361119891,\n",
       " 'bush': 2.386294361119891,\n",
       " 'come': 2.386294361119891,\n",
       " 'cost': 2.386294361119891,\n",
       " 'court': 2.386294361119891,\n",
       " 'doogie': 2.386294361119891,\n",
       " 'fish': 2.386294361119891,\n",
       " 'goes': 2.386294361119891,\n",
       " 'good': 2.386294361119891,\n",
       " 'hand': 2.386294361119891,\n",
       " 'howser': 2.386294361119891,\n",
       " 'in': 1.6931471805599454,\n",
       " 'is': 1.9808292530117262,\n",
       " 'mr': 2.386294361119891,\n",
       " 'other': 2.386294361119891,\n",
       " 'sea': 2.386294361119891,\n",
       " 'smith': 2.386294361119891,\n",
       " 'the': 1.6931471805599454,\n",
       " 'there': 2.386294361119891,\n",
       " 'these': 2.386294361119891,\n",
       " 'things': 2.386294361119891,\n",
       " 'those': 2.386294361119891,\n",
       " 'to': 1.9808292530117262,\n",
       " 'two': 2.386294361119891,\n",
       " 'wait': 2.386294361119891,\n",
       " 'washington': 2.386294361119891,\n",
       " 'watches': 2.386294361119891,\n",
       " 'who': 2.386294361119891,\n",
       " 'worth': 2.386294361119891,\n",
       " 'your': 2.386294361119891}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(tfidf_vectorizer.get_feature_names(), tfidf_vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final scorings of each word from the other words in the vocabulary.\n",
    "* The scores are normalized to values between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.34908308, 0.34908308,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.34908308, 0.        , 0.49536976,\n",
       "        0.28976893, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.24768488, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.34908308, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.34908308, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.38665001, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.38665001, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.38665001, 0.38665001,\n",
       "        0.32095271, 0.        , 0.38665001, 0.        , 0.        ,\n",
       "        0.38665001, 0.        , 0.        ],\n",
       "       [0.5       , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.5       , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.5       , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.5       ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.40801493, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.40801493,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.28949879,\n",
       "        0.        , 0.        , 0.40801493, 0.40801493, 0.        ,\n",
       "        0.28949879, 0.40801493, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.46146654, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.46146654, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.3274243 ,\n",
       "        0.38305686, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.3274243 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.46146654],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.46180424, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.46180424, 0.        , 0.        , 0.46180424,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.38333718, 0.        , 0.        , 0.46180424, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.70710678, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.70710678, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vector2.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t0.3490830767264469\n",
      "  (0, 14)\t0.49536975552604884\n",
      "  (0, 12)\t0.3490830767264469\n",
      "  (0, 15)\t0.2897689326921819\n",
      "  (0, 31)\t0.3490830767264469\n",
      "  (0, 26)\t0.3490830767264469\n",
      "  (0, 20)\t0.24768487776302442\n",
      "  (0, 4)\t0.3490830767264469\n",
      "  (1, 11)\t0.386650005027498\n",
      "  (1, 23)\t0.386650005027498\n",
      "  (1, 5)\t0.386650005027498\n",
      "  (1, 25)\t0.32095270940344806\n",
      "  (1, 24)\t0.386650005027498\n",
      "  (1, 30)\t0.386650005027498\n",
      "  (1, 27)\t0.386650005027498\n",
      "  (2, 22)\t0.5\n",
      "  (2, 29)\t0.5\n",
      "  (2, 6)\t0.5\n",
      "  (2, 0)\t0.5\n",
      "  (3, 14)\t0.2894987873064995\n",
      "  (3, 20)\t0.2894987873064995\n",
      "  (3, 21)\t0.40801492725049476\n",
      "  (3, 1)\t0.40801492725049476\n",
      "  (3, 17)\t0.40801492725049476\n",
      "  (3, 9)\t0.40801492725049476\n",
      "  (3, 18)\t0.40801492725049476\n",
      "  (4, 14)\t0.3274243027464032\n",
      "  (4, 15)\t0.38305685676572565\n",
      "  (4, 20)\t0.3274243027464032\n",
      "  (4, 2)\t0.4614665377636916\n",
      "  (4, 32)\t0.4614665377636916\n",
      "  (4, 7)\t0.4614665377636916\n",
      "  (5, 25)\t0.38333717539523177\n",
      "  (5, 16)\t0.4618042361109319\n",
      "  (5, 19)\t0.4618042361109319\n",
      "  (5, 10)\t0.4618042361109319\n",
      "  (5, 28)\t0.4618042361109319\n",
      "  (6, 8)\t0.7071067811865476\n",
      "  (6, 13)\t0.7071067811865476\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 31)\t0.34908307672644684\n",
      "  (0, 26)\t0.34908307672644684\n",
      "  (0, 20)\t0.2476848777630244\n",
      "  (0, 15)\t0.2897689326921819\n",
      "  (0, 14)\t0.4953697555260488\n",
      "  (0, 12)\t0.34908307672644684\n",
      "  (0, 4)\t0.34908307672644684\n",
      "  (0, 3)\t0.34908307672644684\n",
      "  (1, 30)\t0.386650005027498\n",
      "  (1, 27)\t0.386650005027498\n",
      "  (1, 25)\t0.32095270940344806\n",
      "  (1, 24)\t0.386650005027498\n",
      "  (1, 23)\t0.386650005027498\n",
      "  (1, 11)\t0.386650005027498\n",
      "  (1, 5)\t0.386650005027498\n",
      "  (2, 29)\t0.5\n",
      "  (2, 22)\t0.5\n",
      "  (2, 6)\t0.5\n",
      "  (2, 0)\t0.5\n",
      "  (3, 21)\t0.40801492725049476\n",
      "  (3, 20)\t0.2894987873064995\n",
      "  (3, 18)\t0.40801492725049476\n",
      "  (3, 17)\t0.40801492725049476\n",
      "  (3, 14)\t0.2894987873064995\n",
      "  (3, 9)\t0.40801492725049476\n",
      "  (3, 1)\t0.40801492725049476\n",
      "  (4, 32)\t0.4614665377636916\n",
      "  (4, 20)\t0.3274243027464032\n",
      "  (4, 15)\t0.38305685676572565\n",
      "  (4, 14)\t0.3274243027464032\n",
      "  (4, 7)\t0.4614665377636916\n",
      "  (4, 2)\t0.4614665377636916\n",
      "  (5, 28)\t0.4618042361109319\n",
      "  (5, 25)\t0.38333717539523177\n",
      "  (5, 19)\t0.4618042361109319\n",
      "  (5, 16)\t0.4618042361109319\n",
      "  (5, 10)\t0.4618042361109319\n",
      "  (6, 13)\t0.7071067811865476\n",
      "  (6, 8)\t0.7071067811865476\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vector1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><font color=\"green\"><h1>from file: 05-StopwordAndFrequencyFiltering</h1></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stop Words Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabic', 'azerbaijani', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['إذ',\n",
       " 'إذا',\n",
       " 'إذما',\n",
       " 'إذن',\n",
       " 'أف',\n",
       " 'أقل',\n",
       " 'أكثر',\n",
       " 'ألا',\n",
       " 'إلا',\n",
       " 'التي',\n",
       " 'الذي',\n",
       " 'الذين',\n",
       " 'اللاتي',\n",
       " 'اللائي',\n",
       " 'اللتان',\n",
       " 'اللتيا',\n",
       " 'اللتين',\n",
       " 'اللذان',\n",
       " 'اللذين',\n",
       " 'اللواتي',\n",
       " 'إلى',\n",
       " 'إليك',\n",
       " 'إليكم',\n",
       " 'إليكما',\n",
       " 'إليكن',\n",
       " 'أم',\n",
       " 'أما',\n",
       " 'أما',\n",
       " 'إما',\n",
       " 'أن',\n",
       " 'إن',\n",
       " 'إنا',\n",
       " 'أنا',\n",
       " 'أنت',\n",
       " 'أنتم',\n",
       " 'أنتما',\n",
       " 'أنتن',\n",
       " 'إنما',\n",
       " 'إنه',\n",
       " 'أنى',\n",
       " 'أنى',\n",
       " 'آه',\n",
       " 'آها',\n",
       " 'أو',\n",
       " 'أولاء',\n",
       " 'أولئك',\n",
       " 'أوه',\n",
       " 'آي',\n",
       " 'أي',\n",
       " 'أيها',\n",
       " 'إي',\n",
       " 'أين',\n",
       " 'أين',\n",
       " 'أينما',\n",
       " 'إيه',\n",
       " 'بخ',\n",
       " 'بس',\n",
       " 'بعد',\n",
       " 'بعض',\n",
       " 'بك',\n",
       " 'بكم',\n",
       " 'بكم',\n",
       " 'بكما',\n",
       " 'بكن',\n",
       " 'بل',\n",
       " 'بلى',\n",
       " 'بما',\n",
       " 'بماذا',\n",
       " 'بمن',\n",
       " 'بنا',\n",
       " 'به',\n",
       " 'بها',\n",
       " 'بهم',\n",
       " 'بهما',\n",
       " 'بهن',\n",
       " 'بي',\n",
       " 'بين',\n",
       " 'بيد',\n",
       " 'تلك',\n",
       " 'تلكم',\n",
       " 'تلكما',\n",
       " 'ته',\n",
       " 'تي',\n",
       " 'تين',\n",
       " 'تينك',\n",
       " 'ثم',\n",
       " 'ثمة',\n",
       " 'حاشا',\n",
       " 'حبذا',\n",
       " 'حتى',\n",
       " 'حيث',\n",
       " 'حيثما',\n",
       " 'حين',\n",
       " 'خلا',\n",
       " 'دون',\n",
       " 'ذا',\n",
       " 'ذات',\n",
       " 'ذاك',\n",
       " 'ذان',\n",
       " 'ذانك',\n",
       " 'ذلك',\n",
       " 'ذلكم',\n",
       " 'ذلكما',\n",
       " 'ذلكن',\n",
       " 'ذه',\n",
       " 'ذو',\n",
       " 'ذوا',\n",
       " 'ذواتا',\n",
       " 'ذواتي',\n",
       " 'ذي',\n",
       " 'ذين',\n",
       " 'ذينك',\n",
       " 'ريث',\n",
       " 'سوف',\n",
       " 'سوى',\n",
       " 'شتان',\n",
       " 'عدا',\n",
       " 'عسى',\n",
       " 'عل',\n",
       " 'على',\n",
       " 'عليك',\n",
       " 'عليه',\n",
       " 'عما',\n",
       " 'عن',\n",
       " 'عند',\n",
       " 'غير',\n",
       " 'فإذا',\n",
       " 'فإن',\n",
       " 'فلا',\n",
       " 'فمن',\n",
       " 'في',\n",
       " 'فيم',\n",
       " 'فيما',\n",
       " 'فيه',\n",
       " 'فيها',\n",
       " 'قد',\n",
       " 'كأن',\n",
       " 'كأنما',\n",
       " 'كأي',\n",
       " 'كأين',\n",
       " 'كذا',\n",
       " 'كذلك',\n",
       " 'كل',\n",
       " 'كلا',\n",
       " 'كلاهما',\n",
       " 'كلتا',\n",
       " 'كلما',\n",
       " 'كليكما',\n",
       " 'كليهما',\n",
       " 'كم',\n",
       " 'كم',\n",
       " 'كما',\n",
       " 'كي',\n",
       " 'كيت',\n",
       " 'كيف',\n",
       " 'كيفما',\n",
       " 'لا',\n",
       " 'لاسيما',\n",
       " 'لدى',\n",
       " 'لست',\n",
       " 'لستم',\n",
       " 'لستما',\n",
       " 'لستن',\n",
       " 'لسن',\n",
       " 'لسنا',\n",
       " 'لعل',\n",
       " 'لك',\n",
       " 'لكم',\n",
       " 'لكما',\n",
       " 'لكن',\n",
       " 'لكنما',\n",
       " 'لكي',\n",
       " 'لكيلا',\n",
       " 'لم',\n",
       " 'لما',\n",
       " 'لن',\n",
       " 'لنا',\n",
       " 'له',\n",
       " 'لها',\n",
       " 'لهم',\n",
       " 'لهما',\n",
       " 'لهن',\n",
       " 'لو',\n",
       " 'لولا',\n",
       " 'لوما',\n",
       " 'لي',\n",
       " 'لئن',\n",
       " 'ليت',\n",
       " 'ليس',\n",
       " 'ليسا',\n",
       " 'ليست',\n",
       " 'ليستا',\n",
       " 'ليسوا',\n",
       " 'ما',\n",
       " 'ماذا',\n",
       " 'متى',\n",
       " 'مذ',\n",
       " 'مع',\n",
       " 'مما',\n",
       " 'ممن',\n",
       " 'من',\n",
       " 'منه',\n",
       " 'منها',\n",
       " 'منذ',\n",
       " 'مه',\n",
       " 'مهما',\n",
       " 'نحن',\n",
       " 'نحو',\n",
       " 'نعم',\n",
       " 'ها',\n",
       " 'هاتان',\n",
       " 'هاته',\n",
       " 'هاتي',\n",
       " 'هاتين',\n",
       " 'هاك',\n",
       " 'هاهنا',\n",
       " 'هذا',\n",
       " 'هذان',\n",
       " 'هذه',\n",
       " 'هذي',\n",
       " 'هذين',\n",
       " 'هكذا',\n",
       " 'هل',\n",
       " 'هلا',\n",
       " 'هم',\n",
       " 'هما',\n",
       " 'هن',\n",
       " 'هنا',\n",
       " 'هناك',\n",
       " 'هنالك',\n",
       " 'هو',\n",
       " 'هؤلاء',\n",
       " 'هي',\n",
       " 'هيا',\n",
       " 'هيت',\n",
       " 'هيهات',\n",
       " 'والذي',\n",
       " 'والذين',\n",
       " 'وإذ',\n",
       " 'وإذا',\n",
       " 'وإن',\n",
       " 'ولا',\n",
       " 'ولكن',\n",
       " 'ولو',\n",
       " 'وما',\n",
       " 'ومن',\n",
       " 'وهو',\n",
       " 'يا']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('arabic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A bird in hand is worth two in the bush. Good things come to those who wait. These watches cost $1500!  There are other fish in the sea. The ball is in your court. Mr. Smith Goes to Washington  Doogie Howser M.D.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_array = [\"A bird in hand is worth two in the bush.\",\n",
    "              \"Good things come to those who wait.\",\n",
    "              \"These watches cost $1500! \",\n",
    "              \"There are other fish in the sea.\",\n",
    "              \"The ball is in your court.\",\n",
    "              \"Mr. Smith Goes to Washington \",\n",
    "              \"Doogie Howser M.D.\"]\n",
    "\n",
    "text = \" \".join(text_array)\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'bird',\n",
       " 'in',\n",
       " 'hand',\n",
       " 'is',\n",
       " 'worth',\n",
       " 'two',\n",
       " 'in',\n",
       " 'the',\n",
       " 'bush',\n",
       " '.',\n",
       " 'Good',\n",
       " 'things',\n",
       " 'come',\n",
       " 'to',\n",
       " 'those',\n",
       " 'who',\n",
       " 'wait',\n",
       " '.',\n",
       " 'These',\n",
       " 'watches',\n",
       " 'cost',\n",
       " '$',\n",
       " '1500',\n",
       " '!',\n",
       " 'There',\n",
       " 'are',\n",
       " 'other',\n",
       " 'fish',\n",
       " 'in',\n",
       " 'the',\n",
       " 'sea',\n",
       " '.',\n",
       " 'The',\n",
       " 'ball',\n",
       " 'is',\n",
       " 'in',\n",
       " 'your',\n",
       " 'court',\n",
       " '.',\n",
       " 'Mr.',\n",
       " 'Smith',\n",
       " 'Goes',\n",
       " 'to',\n",
       " 'Washington',\n",
       " 'Doogie',\n",
       " 'Howser',\n",
       " 'M.D',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens = word_tokenize(text)\n",
    "\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'bird', 'hand', 'worth', 'two', 'bush', '.', 'Good', 'things', 'come', 'wait', '.', 'These', 'watches', 'cost', '$', '1500', '!', 'There', 'fish', 'sea', '.', 'The', 'ball', 'court', '.', 'Mr.', 'Smith', 'Goes', 'Washington', 'Doogie', 'Howser', 'M.D', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_words = []\n",
    "\n",
    "for word in word_tokens:\n",
    "    if word not in stop_words:\n",
    "        filtered_words.append(word)\n",
    "        \n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./datasets/file.txt\", \"w\") as f:\n",
    "    for word in filtered_words:\n",
    "        f.write(word)\n",
    "        f.write(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A bird hand worth two bush . Good things come wait . These watches cost $ 1500 ! There fish sea . The ball court . Mr. Smith Goes Washington Doogie Howser M.D . \n"
     ]
    }
   ],
   "source": [
    "with open(\"./datasets/file.txt\", \"r\") as f:\n",
    "    file_contents = f.read()\n",
    "\n",
    "print(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit([file_contents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 25)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_vector = count_vectorizer.transform(text_array)\n",
    "\n",
    "transformed_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_nltk = count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bird': 2,\n",
       " 'hand': 11,\n",
       " 'worth': 24,\n",
       " 'two': 20,\n",
       " 'bush': 3,\n",
       " 'good': 10,\n",
       " 'things': 19,\n",
       " 'come': 4,\n",
       " 'wait': 21,\n",
       " 'these': 18,\n",
       " 'watches': 23,\n",
       " 'cost': 5,\n",
       " '1500': 0,\n",
       " 'there': 17,\n",
       " 'fish': 8,\n",
       " 'sea': 14,\n",
       " 'the': 16,\n",
       " 'ball': 1,\n",
       " 'court': 6,\n",
       " 'mr': 13,\n",
       " 'smith': 15,\n",
       " 'goes': 9,\n",
       " 'washington': 22,\n",
       " 'doogie': 7,\n",
       " 'howser': 12}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "        0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['bird', 'bush', 'hand', 'the', 'two', 'worth'], dtype='<U10'),\n",
       " array(['come', 'good', 'things', 'wait'], dtype='<U10'),\n",
       " array(['1500', 'cost', 'these', 'watches'], dtype='<U10'),\n",
       " array(['fish', 'sea', 'the', 'there'], dtype='<U10'),\n",
       " array(['ball', 'court', 'the'], dtype='<U10'),\n",
       " array(['goes', 'mr', 'smith', 'washington'], dtype='<U10'),\n",
       " array(['doogie', 'howser'], dtype='<U10')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.inverse_transform(transformed_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stop Words Using sklearn\n",
    "\n",
    "The stop_words='english' parameter is not recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 21)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "transformed_vector = count_vectorizer.fit_transform(text_array)\n",
    "\n",
    "transformed_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_sklearn = count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['bush', 'worth', 'hand', 'bird'], dtype='<U10'),\n",
       " array(['wait', 'come', 'things', 'good'], dtype='<U10'),\n",
       " array(['1500', 'cost', 'watches'], dtype='<U10'),\n",
       " array(['sea', 'fish'], dtype='<U10'),\n",
       " array(['court', 'ball'], dtype='<U10'),\n",
       " array(['washington', 'goes', 'smith', 'mr'], dtype='<U10'),\n",
       " array(['howser', 'doogie'], dtype='<U10')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.inverse_transform(transformed_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set difference of both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_diff(first, second):\n",
    "        second = set(second)\n",
    "        return [item for item in first if item not in second]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_diff(feature_names_sklearn, feature_names_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'there', 'these', 'two']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_diff(feature_names_nltk, feature_names_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering words based on frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn-feature-extraction-text-countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "transformed_vector = count_vectorizer.fit_transform(newsgroups.data)\n",
    "\n",
    "transformed_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130094)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(min_df=0.0, max_df=0.7)\n",
    "\n",
    "transformed_vector = count_vectorizer.fit_transform(newsgroups.data)\n",
    "\n",
    "transformed_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 2155)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(min_df=0.01, max_df=0.7)\n",
    "\n",
    "transformed_vector = count_vectorizer.fit_transform(newsgroups.data)\n",
    "\n",
    "transformed_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 127701)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(min_df=0, max_df=100)\n",
    "\n",
    "transformed_vector = count_vectorizer.fit_transform(newsgroups.data)\n",
    "\n",
    "transformed_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 54030)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(min_df=2, max_df=100)\n",
    "\n",
    "transformed_vector = count_vectorizer.fit_transform(newsgroups.data)\n",
    "\n",
    "transformed_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><font color=\"green\"><h1>from file: 06-Stemming</h1></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming Words with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import *\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PorterStemmer\n",
    "* PorterStemmer uses Suffix Stripping to produce stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = ['overwhelming', 'overwhelmingly', \n",
    "                'hushed', 'hush',\n",
    "                'functional', 'functionally',\n",
    "                'lying', 'lied',\n",
    "                'fairly', \n",
    "                'destabilize', 'stability',\n",
    "                'friendship', 'friendships', 'friendly', 'friendless', \n",
    "                'connect', 'connections', 'connected',  \n",
    "                'the', 'these', 'those',\n",
    "                'motivational', 'motivate', 'motivating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "ps_stemmed_tokens = []\n",
    "for token in input_tokens:\n",
    "    ps_stemmed_tokens.append(ps.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>Porter Stemmer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>overwhelming</td>\n",
       "      <td>overwhelm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>overwhelmingly</td>\n",
       "      <td>overwhelmingli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hushed</td>\n",
       "      <td>hush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hush</td>\n",
       "      <td>hush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>functional</td>\n",
       "      <td>function</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>functionally</td>\n",
       "      <td>function</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lying</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lied</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fairly</td>\n",
       "      <td>fairli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>destabilize</td>\n",
       "      <td>destabil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stability</td>\n",
       "      <td>stabil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>friendship</td>\n",
       "      <td>friendship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>friendships</td>\n",
       "      <td>friendship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>friendly</td>\n",
       "      <td>friendli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>friendless</td>\n",
       "      <td>friendless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>connect</td>\n",
       "      <td>connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>connections</td>\n",
       "      <td>connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>connected</td>\n",
       "      <td>connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>these</td>\n",
       "      <td>these</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>those</td>\n",
       "      <td>those</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>motivational</td>\n",
       "      <td>motiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>motivate</td>\n",
       "      <td>motiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>motivating</td>\n",
       "      <td>motiv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             words  Porter Stemmer\n",
       "0     overwhelming       overwhelm\n",
       "1   overwhelmingly  overwhelmingli\n",
       "2           hushed            hush\n",
       "3             hush            hush\n",
       "4       functional        function\n",
       "5     functionally        function\n",
       "6            lying             lie\n",
       "7             lied             lie\n",
       "8           fairly          fairli\n",
       "9      destabilize        destabil\n",
       "10       stability          stabil\n",
       "11      friendship      friendship\n",
       "12     friendships      friendship\n",
       "13        friendly        friendli\n",
       "14      friendless      friendless\n",
       "15         connect         connect\n",
       "16     connections         connect\n",
       "17       connected         connect\n",
       "18             the             the\n",
       "19           these           these\n",
       "20           those           those\n",
       "21    motivational           motiv\n",
       "22        motivate           motiv\n",
       "23      motivating           motiv"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stems_df = pd.DataFrame({\n",
    "    'words': input_tokens,\n",
    "    'Porter Stemmer': ps_stemmed_tokens\n",
    "})\n",
    "\n",
    "stems_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LancasterStemmer\n",
    "* The LancasterStemmer (Paice-Husk stemmer) is an iterative algorithm with rules saved externally.\n",
    "* LancasterStemmer is simple, but heavy stemming due to iterations and over-stemming may occur. \n",
    "* Over-stemming causes the stems to be not linguistic, or they may have no meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = LancasterStemmer()\n",
    "\n",
    "ls_stemmed_tokens = []\n",
    "for token in input_tokens:\n",
    "    ls_stemmed_tokens.append(ls.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>Lancaster Stemmer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>overwhelming</td>\n",
       "      <td>overwhelm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>overwhelmingly</td>\n",
       "      <td>overwhelm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hushed</td>\n",
       "      <td>hush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hush</td>\n",
       "      <td>hush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>functional</td>\n",
       "      <td>funct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>functionally</td>\n",
       "      <td>funct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lying</td>\n",
       "      <td>lying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lied</td>\n",
       "      <td>lied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fairly</td>\n",
       "      <td>fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>destabilize</td>\n",
       "      <td>dest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stability</td>\n",
       "      <td>stabl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>friendship</td>\n",
       "      <td>friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>friendships</td>\n",
       "      <td>friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>friendly</td>\n",
       "      <td>friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>friendless</td>\n",
       "      <td>friendless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>connect</td>\n",
       "      <td>connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>connections</td>\n",
       "      <td>connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>connected</td>\n",
       "      <td>connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>these</td>\n",
       "      <td>thes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>those</td>\n",
       "      <td>thos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>motivational</td>\n",
       "      <td>mot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>motivate</td>\n",
       "      <td>mot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>motivating</td>\n",
       "      <td>mot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             words Lancaster Stemmer\n",
       "0     overwhelming         overwhelm\n",
       "1   overwhelmingly         overwhelm\n",
       "2           hushed              hush\n",
       "3             hush              hush\n",
       "4       functional             funct\n",
       "5     functionally             funct\n",
       "6            lying             lying\n",
       "7             lied              lied\n",
       "8           fairly              fair\n",
       "9      destabilize              dest\n",
       "10       stability             stabl\n",
       "11      friendship            friend\n",
       "12     friendships            friend\n",
       "13        friendly            friend\n",
       "14      friendless        friendless\n",
       "15         connect           connect\n",
       "16     connections           connect\n",
       "17       connected           connect\n",
       "18             the               the\n",
       "19           these              thes\n",
       "20           those              thos\n",
       "21    motivational               mot\n",
       "22        motivate               mot\n",
       "23      motivating               mot"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stems_df = pd.DataFrame({\n",
    "    'words': input_tokens,\n",
    "    'Lancaster Stemmer': ls_stemmed_tokens\n",
    "})\n",
    "\n",
    "stems_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>Porter Stemmer</th>\n",
       "      <th>Lancaster Stemmer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>overwhelming</td>\n",
       "      <td>overwhelm</td>\n",
       "      <td>overwhelm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>overwhelmingly</td>\n",
       "      <td>overwhelmingli</td>\n",
       "      <td>overwhelm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hushed</td>\n",
       "      <td>hush</td>\n",
       "      <td>hush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hush</td>\n",
       "      <td>hush</td>\n",
       "      <td>hush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>functional</td>\n",
       "      <td>function</td>\n",
       "      <td>funct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>functionally</td>\n",
       "      <td>function</td>\n",
       "      <td>funct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lying</td>\n",
       "      <td>lie</td>\n",
       "      <td>lying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lied</td>\n",
       "      <td>lie</td>\n",
       "      <td>lied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fairly</td>\n",
       "      <td>fairli</td>\n",
       "      <td>fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>destabilize</td>\n",
       "      <td>destabil</td>\n",
       "      <td>dest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stability</td>\n",
       "      <td>stabil</td>\n",
       "      <td>stabl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>friendship</td>\n",
       "      <td>friendship</td>\n",
       "      <td>friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>friendships</td>\n",
       "      <td>friendship</td>\n",
       "      <td>friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>friendly</td>\n",
       "      <td>friendli</td>\n",
       "      <td>friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>friendless</td>\n",
       "      <td>friendless</td>\n",
       "      <td>friendless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>connect</td>\n",
       "      <td>connect</td>\n",
       "      <td>connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>connections</td>\n",
       "      <td>connect</td>\n",
       "      <td>connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>connected</td>\n",
       "      <td>connect</td>\n",
       "      <td>connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>these</td>\n",
       "      <td>these</td>\n",
       "      <td>thes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>those</td>\n",
       "      <td>those</td>\n",
       "      <td>thos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>motivational</td>\n",
       "      <td>motiv</td>\n",
       "      <td>mot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>motivate</td>\n",
       "      <td>motiv</td>\n",
       "      <td>mot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>motivating</td>\n",
       "      <td>motiv</td>\n",
       "      <td>mot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             words  Porter Stemmer Lancaster Stemmer\n",
       "0     overwhelming       overwhelm         overwhelm\n",
       "1   overwhelmingly  overwhelmingli         overwhelm\n",
       "2           hushed            hush              hush\n",
       "3             hush            hush              hush\n",
       "4       functional        function             funct\n",
       "5     functionally        function             funct\n",
       "6            lying             lie             lying\n",
       "7             lied             lie              lied\n",
       "8           fairly          fairli              fair\n",
       "9      destabilize        destabil              dest\n",
       "10       stability          stabil             stabl\n",
       "11      friendship      friendship            friend\n",
       "12     friendships      friendship            friend\n",
       "13        friendly        friendli            friend\n",
       "14      friendless      friendless        friendless\n",
       "15         connect         connect           connect\n",
       "16     connections         connect           connect\n",
       "17       connected         connect           connect\n",
       "18             the             the               the\n",
       "19           these           these              thes\n",
       "20           those           those              thos\n",
       "21    motivational           motiv               mot\n",
       "22        motivate           motiv               mot\n",
       "23      motivating           motiv               mot"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stems_df = pd.DataFrame({\n",
    "    'words': input_tokens,\n",
    "    'Porter Stemmer': ps_stemmed_tokens,\n",
    "    'Lancaster Stemmer': ls_stemmed_tokens\n",
    "})\n",
    "\n",
    "stems_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SnowballStemmer\n",
    "* One can generate its own set of rules for any language that is why Python nltk introduced SnowballStemmers that are used to create non-English Stemmers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
     ]
    }
   ],
   "source": [
    "print(SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss =  SnowballStemmer('english')\n",
    "\n",
    "ss_stemmed_tokens = []\n",
    "for token in input_tokens:\n",
    "    ss_stemmed_tokens.append(ss.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>Snowball Stemmer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>overwhelming</td>\n",
       "      <td>overwhelm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>overwhelmingly</td>\n",
       "      <td>overwhelm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hushed</td>\n",
       "      <td>hush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hush</td>\n",
       "      <td>hush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>functional</td>\n",
       "      <td>function</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>functionally</td>\n",
       "      <td>function</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lying</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lied</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fairly</td>\n",
       "      <td>fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>destabilize</td>\n",
       "      <td>destabil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stability</td>\n",
       "      <td>stabil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>friendship</td>\n",
       "      <td>friendship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>friendships</td>\n",
       "      <td>friendship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>friendly</td>\n",
       "      <td>friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>friendless</td>\n",
       "      <td>friendless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>connect</td>\n",
       "      <td>connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>connections</td>\n",
       "      <td>connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>connected</td>\n",
       "      <td>connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>these</td>\n",
       "      <td>these</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>those</td>\n",
       "      <td>those</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>motivational</td>\n",
       "      <td>motiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>motivate</td>\n",
       "      <td>motiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>motivating</td>\n",
       "      <td>motiv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             words Snowball Stemmer\n",
       "0     overwhelming        overwhelm\n",
       "1   overwhelmingly        overwhelm\n",
       "2           hushed             hush\n",
       "3             hush             hush\n",
       "4       functional         function\n",
       "5     functionally         function\n",
       "6            lying              lie\n",
       "7             lied              lie\n",
       "8           fairly             fair\n",
       "9      destabilize         destabil\n",
       "10       stability           stabil\n",
       "11      friendship       friendship\n",
       "12     friendships       friendship\n",
       "13        friendly           friend\n",
       "14      friendless       friendless\n",
       "15         connect          connect\n",
       "16     connections          connect\n",
       "17       connected          connect\n",
       "18             the              the\n",
       "19           these            these\n",
       "20           those            those\n",
       "21    motivational            motiv\n",
       "22        motivate            motiv\n",
       "23      motivating            motiv"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stems_df = pd.DataFrame({\n",
    "    'words': input_tokens,\n",
    "    'Snowball Stemmer': ss_stemmed_tokens\n",
    "})\n",
    "\n",
    "stems_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>Porter Stemmer</th>\n",
       "      <th>Lancaster Stemmer</th>\n",
       "      <th>Snowball Stemmer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>overwhelming</td>\n",
       "      <td>overwhelm</td>\n",
       "      <td>overwhelm</td>\n",
       "      <td>overwhelm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>overwhelmingly</td>\n",
       "      <td>overwhelmingli</td>\n",
       "      <td>overwhelm</td>\n",
       "      <td>overwhelm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hushed</td>\n",
       "      <td>hush</td>\n",
       "      <td>hush</td>\n",
       "      <td>hush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hush</td>\n",
       "      <td>hush</td>\n",
       "      <td>hush</td>\n",
       "      <td>hush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>functional</td>\n",
       "      <td>function</td>\n",
       "      <td>funct</td>\n",
       "      <td>function</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>functionally</td>\n",
       "      <td>function</td>\n",
       "      <td>funct</td>\n",
       "      <td>function</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lying</td>\n",
       "      <td>lie</td>\n",
       "      <td>lying</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lied</td>\n",
       "      <td>lie</td>\n",
       "      <td>lied</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fairly</td>\n",
       "      <td>fairli</td>\n",
       "      <td>fair</td>\n",
       "      <td>fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>destabilize</td>\n",
       "      <td>destabil</td>\n",
       "      <td>dest</td>\n",
       "      <td>destabil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stability</td>\n",
       "      <td>stabil</td>\n",
       "      <td>stabl</td>\n",
       "      <td>stabil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>friendship</td>\n",
       "      <td>friendship</td>\n",
       "      <td>friend</td>\n",
       "      <td>friendship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>friendships</td>\n",
       "      <td>friendship</td>\n",
       "      <td>friend</td>\n",
       "      <td>friendship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>friendly</td>\n",
       "      <td>friendli</td>\n",
       "      <td>friend</td>\n",
       "      <td>friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>friendless</td>\n",
       "      <td>friendless</td>\n",
       "      <td>friendless</td>\n",
       "      <td>friendless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>connect</td>\n",
       "      <td>connect</td>\n",
       "      <td>connect</td>\n",
       "      <td>connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>connections</td>\n",
       "      <td>connect</td>\n",
       "      <td>connect</td>\n",
       "      <td>connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>connected</td>\n",
       "      <td>connect</td>\n",
       "      <td>connect</td>\n",
       "      <td>connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>these</td>\n",
       "      <td>these</td>\n",
       "      <td>thes</td>\n",
       "      <td>these</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>those</td>\n",
       "      <td>those</td>\n",
       "      <td>thos</td>\n",
       "      <td>those</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>motivational</td>\n",
       "      <td>motiv</td>\n",
       "      <td>mot</td>\n",
       "      <td>motiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>motivate</td>\n",
       "      <td>motiv</td>\n",
       "      <td>mot</td>\n",
       "      <td>motiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>motivating</td>\n",
       "      <td>motiv</td>\n",
       "      <td>mot</td>\n",
       "      <td>motiv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             words  Porter Stemmer Lancaster Stemmer Snowball Stemmer\n",
       "0     overwhelming       overwhelm         overwhelm        overwhelm\n",
       "1   overwhelmingly  overwhelmingli         overwhelm        overwhelm\n",
       "2           hushed            hush              hush             hush\n",
       "3             hush            hush              hush             hush\n",
       "4       functional        function             funct         function\n",
       "5     functionally        function             funct         function\n",
       "6            lying             lie             lying              lie\n",
       "7             lied             lie              lied              lie\n",
       "8           fairly          fairli              fair             fair\n",
       "9      destabilize        destabil              dest         destabil\n",
       "10       stability          stabil             stabl           stabil\n",
       "11      friendship      friendship            friend       friendship\n",
       "12     friendships      friendship            friend       friendship\n",
       "13        friendly        friendli            friend           friend\n",
       "14      friendless      friendless        friendless       friendless\n",
       "15         connect         connect           connect          connect\n",
       "16     connections         connect           connect          connect\n",
       "17       connected         connect           connect          connect\n",
       "18             the             the               the              the\n",
       "19           these           these              thes            these\n",
       "20           those           those              thos            those\n",
       "21    motivational           motiv               mot            motiv\n",
       "22        motivate           motiv               mot            motiv\n",
       "23      motivating           motiv               mot            motiv"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stems_df = pd.DataFrame({\n",
    "    'words': input_tokens,\n",
    "    'Porter Stemmer': ps_stemmed_tokens,\n",
    "    'Lancaster Stemmer': ls_stemmed_tokens,\n",
    "    'Snowball Stemmer': ss_stemmed_tokens\n",
    "})\n",
    "\n",
    "stems_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suffix stripping algorithms may differ in results for a variety of reasons. One such reason is whether the algorithm constrains whether the output word must be a real word in the given language. Some approaches do not require the word to actually exist in the language lexicon (the set of all words in the language). Alternatively, some suffix stripping approaches maintain a database (a large list) of all known morphological word roots that exist as real words. These approaches check the list for the existence of the term prior to making a decision. Typically, if the term does not exist, alternate action is taken. This alternate action may involve several other criteria. The non-existence of an output term may serve to cause the algorithm to try alternate suffix stripping rules.\n",
      "\n",
      "It can be the case that two or more suffix stripping rules apply to the same input term, which creates an ambiguity as to which rule to apply. The algorithm may assign (by human hand or stochastically) a priority to one rule or another. Or the algorithm may reject one rule application because it results in a non-existent term whereas the other overlapping rule does not. For example, given the English term friendlies, the algorithm may identify the ies suffix and apply the appropriate rule and achieve the result of friendl. friendl is likely not found in the lexicon, and therefore the rule is rejected.\n",
      "\n",
      "One improvement upon basic suffix stripping is the use of suffix substitution. Similar to a stripping rule, a substitution rule replaces a suffix with an alternate suffix. For example, there could exist a rule that replaces ies with y. How this affects the algorithm varies on the algorithm's design. To illustrate, the algorithm may identify that both the ies suffix stripping rule as well as the suffix substitution rule apply. Since the stripping rule results in a non-existent term in the lexicon, but the substitution rule does not, the substitution rule is applied instead. In this example, friendlies becomes friendly instead of friendl.\n",
      "\n",
      "Diving further into the details, a common technique is to apply rules in a cyclical fashion (recursively, as computer scientists would say). After applying the suffix substitution rule in this example scenario, a second pass is made to identify matching rules on the term friendly, where the ly stripping rule is likely identified and accepted. In summary, friendlies becomes (via substitution) friendly which becomes (via stripping) friend.\n",
      "\n",
      "This example also helps illustrate the difference between a rule-based approach and a brute force approach. In a brute force approach, the algorithm would search for friendlies in the set of hundreds of thousands of inflected word forms and ideally find the corresponding root form friend. In the rule-based approach, the three rules mentioned above would be applied in succession to converge on the same solution. Chances are that the rule-based approach would be slower, as lookup algorithms have a direct access to the solution, while rule-based should try several options, and combinations of them, and then choose which result seems to be the best.\n",
      "\n",
      "There are two error measurements in stemming algorithms, overstemming and understemming. Overstemming is an error where two separate inflected words are stemmed to the same root, but should not have been—a false positive. Understemming is an error where two separate inflected words should be stemmed to the same root, but are not—a false negative. Stemming algorithms attempt to minimize each type of error, although reducing one type can lead to increasing the other.\n",
      "\n",
      "For example, the widely used Porter stemmer stems \"universal\", \"university\", and \"universe\" to \"univers\". This is a case of overstemming: though these three words are etymologically related, their modern meanings are in widely different domains, so treating them as synonyms in a search engine will likely reduce the relevance of the search results.\n",
      "\n",
      "An example of understemming in the Porter stemmer is \"alumnus\" → \"alumnu\", \"alumni\" → \"alumni\", \"alumna\"/\"alumnae\" → \"alumna\". This English word keeps Latin morphology, and so these near-synonyms are not conflated.\n",
      "\n",
      "Stochastic algorithms involve using probability to identify the root form of a word. Stochastic algorithms are trained (they \"learn\") on a table of root form to inflected form relations to develop a probabilistic model. This model is typically expressed in the form of complex linguistic rules, similar in nature to those in suffix stripping or lemmatisation. Stemming is performed by inputting an inflected form to the trained model and having the model produce the root form according to its internal ruleset, which again is similar to suffix stripping and lemmatisation, except that the decisions involved in applying the most appropriate rule, or whether or not to stem the word and just return the same word, or whether to apply two different rules sequentially, are applied on the grounds that the output word will have the highest probability of being correct (which is to say, the smallest probability of being incorrect, which is how it is typically measured).\n",
      "\n",
      "Some lemmatisation algorithms are stochastic in that, given a word which may belong to multiple parts of speech, a probability is assigned to each possible part. This may take into account the surrounding words, called the context, or not. Context-free grammars do not take into account any additional information. In either case, after assigning the probabilities to each possible part of speech, the most likely part of speech is chosen, and from there the appropriate normalization rules are applied to the input word to produce the normalized (root) form.\n"
     ]
    }
   ],
   "source": [
    "with open('./datasets/stemming.txt', 'r') as f:\n",
    "    file_contents = f.read()\n",
    "\n",
    "print(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = word_tokenize(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss =  SnowballStemmer('english', ignore_stopwords=True)\n",
    "\n",
    "ss_stemmed_words = []\n",
    "for word in word_tokens:\n",
    "    ss_stemmed_words.append(ss.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"suffix strip algorithm may differ in result for a varieti of reason . one such reason is whether the algorithm constrain whether the output word must be a real word in the given languag . some approach do not requir the word to actual exist in the languag lexicon ( the set of all word in the languag ) . altern , some suffix strip approach maintain a databas ( a larg list ) of all known morpholog word root that exist as real word . these approach check the list for the exist of the term prior to make a decis . typic , if the term does not exist , altern action is taken . this altern action may involv sever other criteria . the non-exist of an output term may serv to caus the algorithm to tri altern suffix strip rule . it can be the case that two or more suffix strip rule appli to the same input term , which creat an ambigu as to which rule to appli . the algorithm may assign ( by human hand or stochast ) a prioriti to one rule or anoth . or the algorithm may reject one rule applic because it result in a non-exist term wherea the other overlap rule does not . for exampl , given the english term friend , the algorithm may identifi the ie suffix and appli the appropri rule and achiev the result of friendl . friendl is like not found in the lexicon , and therefor the rule is reject . one improv upon basic suffix strip is the use of suffix substitut . similar to a strip rule , a substitut rule replac a suffix with an altern suffix . for exampl , there could exist a rule that replac ie with y . how this affect the algorithm vari on the algorithm 's design . to illustr , the algorithm may identifi that both the ie suffix strip rule as well as the suffix substitut rule appli . sinc the strip rule result in a non-exist term in the lexicon , but the substitut rule does not , the substitut rule is appli instead . in this exampl , friend becom friend instead of friendl . dive further into the detail , a common techniqu is to appli rule in a cyclic fashion ( recurs , as comput scientist would say ) . after appli the suffix substitut rule in this exampl scenario , a second pass is made to identifi match rule on the term friend , where the ly strip rule is like identifi and accept . in summari , friend becom ( via substitut ) friend which becom ( via strip ) friend . this exampl also help illustr the differ between a rule-bas approach and a brute forc approach . in a brute forc approach , the algorithm would search for friend in the set of hundr of thousand of inflect word form and ideal find the correspond root form friend . in the rule-bas approach , the three rule mention above would be appli in success to converg on the same solut . chanc are that the rule-bas approach would be slower , as lookup algorithm have a direct access to the solut , while rule-bas should tri sever option , and combin of them , and then choos which result seem to be the best . there are two error measur in stem algorithm , overstem and understem . overstem is an error where two separ inflect word are stem to the same root , but should not have been—a fals posit . understem is an error where two separ inflect word should be stem to the same root , but are not—a fals negat . stem algorithm attempt to minim each type of error , although reduc one type can lead to increas the other . for exampl , the wide use porter stemmer stem `` univers '' , `` univers '' , and `` univers '' to `` univ '' . this is a case of overstem : though these three word are etymolog relat , their modern mean are in wide differ domain , so treat them as synonym in a search engin will like reduc the relev of the search result . an exampl of understem in the porter stemmer is `` alumnus '' → `` alumnu '' , `` alumni '' → `` alumni '' , `` alumna '' / '' alumna '' → `` alumna '' . this english word keep latin morpholog , and so these near-synonym are not conflat . stochast algorithm involv use probabl to identifi the root form of a word . stochast algorithm are train ( they `` learn '' ) on a tabl of root form to inflect form relat to develop a probabilist model . this model is typic express in the form of complex linguist rule , similar in natur to those in suffix strip or lemmatis . stem is perform by input an inflect form to the train model and having the model produc the root form accord to its intern ruleset , which again is similar to suffix strip and lemmatis , except that the decis involv in appli the most appropri rule , or whether or not to stem the word and just return the same word , or whether to appli two differ rule sequenti , are appli on the ground that the output word will have the highest probabl of being correct ( which is to say , the smallest probabl of being incorrect , which is how it is typic measur ) . some lemmatis algorithm are stochast in that , given a word which may belong to multipl part of speech , a probabl is assign to each possibl part . this may take into account the surround word , call the context , or not . context-fre grammar do not take into account any addit inform . in either case , after assign the probabl to each possibl part of speech , the most like part of speech is chosen , and from there the appropri normal rule are appli to the input word to produc the normal ( root ) form .\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(ss_stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><font color=\"green\"><h1>from file: 07-Lemmatization</h1></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing Words Using WordNet\n",
    "* Part-of-speech constants:\n",
    "* ADJ: a\n",
    "* ADV: r\n",
    "* NOUN: n\n",
    "* VERB: v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem import *\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definit\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "stemmer = PorterStemmer() \n",
    "print(stemmer.stem('definitions'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/loonycorn/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definition\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "print(wnl.lemmatize('definitions'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing words by specifying parts-of-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjective:  running\n",
      "Adverb:  running\n",
      "Noun:  running\n",
      "Verb:  run\n"
     ]
    }
   ],
   "source": [
    "print('Adjective: ', wnl.lemmatize('running', pos='a'))\n",
    "print('Adverb: ', wnl.lemmatize('running', pos='r'))\n",
    "print('Noun: ', wnl.lemmatize('running', pos='n'))\n",
    "print('Verb: ', wnl.lemmatize('running', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = ['dictionaries', 'dictionary', \n",
    "                'hushed', 'hush', 'hushing',\n",
    "                'functional', 'functionally',\n",
    "                'lying', 'lied', 'lies',\n",
    "                'flawed', 'flaws', 'flawless', \n",
    "                'friendship', 'friendships', 'friendly', 'friendless', \n",
    "                'definitions', 'definition', 'definitely',  \n",
    "                'the', 'these', 'those',\n",
    "                'motivational', 'motivate', 'motivating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss =  SnowballStemmer('english')\n",
    "\n",
    "ss_stemmed_tokens = []\n",
    "for token in input_tokens:\n",
    "    ss_stemmed_tokens.append(ss.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl_lemmatized_tokens = []\n",
    "for token in input_tokens:\n",
    "    wnl_lemmatized_tokens.append(wnl.lemmatize(token, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>Snowball Stemmer</th>\n",
       "      <th>WordNet Lemmatizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dictionaries</td>\n",
       "      <td>dictionari</td>\n",
       "      <td>dictionaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dictionary</td>\n",
       "      <td>dictionari</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hushed</td>\n",
       "      <td>hush</td>\n",
       "      <td>hush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hush</td>\n",
       "      <td>hush</td>\n",
       "      <td>hush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hushing</td>\n",
       "      <td>hush</td>\n",
       "      <td>hush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>functional</td>\n",
       "      <td>function</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>functionally</td>\n",
       "      <td>function</td>\n",
       "      <td>functionally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lying</td>\n",
       "      <td>lie</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lied</td>\n",
       "      <td>lie</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lies</td>\n",
       "      <td>lie</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>flawed</td>\n",
       "      <td>flaw</td>\n",
       "      <td>flaw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>flaws</td>\n",
       "      <td>flaw</td>\n",
       "      <td>flaw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>flawless</td>\n",
       "      <td>flawless</td>\n",
       "      <td>flawless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>friendship</td>\n",
       "      <td>friendship</td>\n",
       "      <td>friendship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>friendships</td>\n",
       "      <td>friendship</td>\n",
       "      <td>friendships</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>friendly</td>\n",
       "      <td>friend</td>\n",
       "      <td>friendly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>friendless</td>\n",
       "      <td>friendless</td>\n",
       "      <td>friendless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>definitions</td>\n",
       "      <td>definit</td>\n",
       "      <td>definitions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>definition</td>\n",
       "      <td>definit</td>\n",
       "      <td>definition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>definitely</td>\n",
       "      <td>definit</td>\n",
       "      <td>definitely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>these</td>\n",
       "      <td>these</td>\n",
       "      <td>these</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>those</td>\n",
       "      <td>those</td>\n",
       "      <td>those</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>motivational</td>\n",
       "      <td>motiv</td>\n",
       "      <td>motivational</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>motivate</td>\n",
       "      <td>motiv</td>\n",
       "      <td>motivate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>motivating</td>\n",
       "      <td>motiv</td>\n",
       "      <td>motivate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           words Snowball Stemmer WordNet Lemmatizer\n",
       "0   dictionaries       dictionari       dictionaries\n",
       "1     dictionary       dictionari         dictionary\n",
       "2         hushed             hush               hush\n",
       "3           hush             hush               hush\n",
       "4        hushing             hush               hush\n",
       "5     functional         function         functional\n",
       "6   functionally         function       functionally\n",
       "7          lying              lie                lie\n",
       "8           lied              lie                lie\n",
       "9           lies              lie                lie\n",
       "10        flawed             flaw               flaw\n",
       "11         flaws             flaw               flaw\n",
       "12      flawless         flawless           flawless\n",
       "13    friendship       friendship         friendship\n",
       "14   friendships       friendship        friendships\n",
       "15      friendly           friend           friendly\n",
       "16    friendless       friendless         friendless\n",
       "17   definitions          definit        definitions\n",
       "18    definition          definit         definition\n",
       "19    definitely          definit         definitely\n",
       "20           the              the                the\n",
       "21         these            these              these\n",
       "22         those            those              those\n",
       "23  motivational            motiv       motivational\n",
       "24      motivate            motiv           motivate\n",
       "25    motivating            motiv           motivate"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stems_lemmas_df = pd.DataFrame({\n",
    "    'words': input_tokens,\n",
    "    'Snowball Stemmer': ss_stemmed_tokens,\n",
    "    'WordNet Lemmatizer': wnl_lemmatized_tokens\n",
    "})\n",
    "\n",
    "stems_lemmas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marie Curie was a Polish-born physicist and chemist and one of the most famous scientists of her time.\n",
      "Together with her husband Pierre, she was awarded the Nobel Prize in 1903, and she went on to win another in 1911.\n",
      "Marie Sklodowska was born in Warsaw on 7 November 1867, the daughter of a teacher.\n",
      "In 1891, she went to Paris to study physics and mathematics at the Sorbonne where she met Pierre Curie, professor of the School of Physics.\n",
      "They were married in 1895.\n",
      "The Curies worked together investigating radioactivity, building on the work of the German physicist Roentgen and the French physicist Becquerel.\n",
      "In July 1898, the Curies announced the discovery of a new chemical element, polonium.\n",
      "At the end of the year, they announced the discovery of another, radium.\n",
      "The Curies, along with Becquerel, were awarded the Nobel Prize for Physics in 1903.\n",
      "Pierre's life was cut short in 1906 when he was knocked down and killed by a carriage.\n",
      "Marie took over his teaching post, becoming the first woman to teach at the Sorbonne, and devoted herself to continuing the work that they had begun together.\n",
      "She received a second Nobel Prize, for Chemistry, in 1911.\n",
      "The Curie's research was crucial in the development of x-rays in surgery.\n",
      "During World War One Curie helped to equip ambulances with x-ray equipment, which she herself drove to the front lines.\n",
      "The International Red Cross made her head of its radiological service and she held training courses for medical orderlies and doctors in the new techniques.\n",
      "Despite her success, Marie continued to face great opposition from male scientists in France, and she never received significant financial benefits from her work.\n",
      "By the late 1920s her health was beginning to deteriorate.\n",
      "She died on 4 July 1934 from leukaemia, caused by exposure to high-energy radiation from her research.\n",
      "The Curies' eldest daughter Irene was herself a scientist and winner of the Nobel Prize for Chemistry.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "with open('./datasets/biography.txt', 'r') as f:\n",
    "    file_contents = f.read()\n",
    "    \n",
    "print(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = word_tokenize(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "lemmatized_words = []\n",
    "\n",
    "for word in word_tokens:\n",
    "    lemmatized_words.append(wnl.lemmatize(word, pos=\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Marie Curie be a Polish-born physicist and chemist and one of the most famous scientists of her time . Together with her husband Pierre , she be award the Nobel Prize in 1903 , and she go on to win another in 1911 . Marie Sklodowska be bear in Warsaw on 7 November 1867 , the daughter of a teacher . In 1891 , she go to Paris to study physics and mathematics at the Sorbonne where she meet Pierre Curie , professor of the School of Physics . They be marry in 1895 . The Curies work together investigate radioactivity , build on the work of the German physicist Roentgen and the French physicist Becquerel . In July 1898 , the Curies announce the discovery of a new chemical element , polonium . At the end of the year , they announce the discovery of another , radium . The Curies , along with Becquerel , be award the Nobel Prize for Physics in 1903 . Pierre 's life be cut short in 1906 when he be knock down and kill by a carriage . Marie take over his teach post , become the first woman to teach at the Sorbonne , and devote herself to continue the work that they have begin together . She receive a second Nobel Prize , for Chemistry , in 1911 . The Curie 's research be crucial in the development of x-ray in surgery . During World War One Curie help to equip ambulances with x-ray equipment , which she herself drive to the front line . The International Red Cross make her head of its radiological service and she hold train course for medical orderlies and doctor in the new techniques . Despite her success , Marie continue to face great opposition from male scientists in France , and she never receive significant financial benefit from her work . By the late 1920s her health be begin to deteriorate . She die on 4 July 1934 from leukaemia , cause by exposure to high-energy radiation from her research . The Curies ' eldest daughter Irene be herself a scientist and winner of the Nobel Prize for Chemistry .\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><font color=\"green\"><h1>from file: 08-PartOfSpeechTagging</h1></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/loonycorn/nltk_data...\n",
      "[nltk_data]   Unzipping help/tagsets.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/loonycorn/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('refuse', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('let', 'VB'),\n",
       " ('this', 'DT'),\n",
       " ('refuse', 'NN'),\n",
       " ('get', 'VB'),\n",
       " ('me', 'PRP'),\n",
       " ('down', 'RP')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I refuse to let this refuse get me down\"\n",
    "\n",
    "tokenized_words = word_tokenize(text)\n",
    "tagged_words = nltk.pos_tag(tokenized_words)\n",
    "tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bear', 'NNP'),\n",
       " ('with', 'IN'),\n",
       " ('me', 'PRP'),\n",
       " (',', ','),\n",
       " ('this', 'DT'),\n",
       " ('effort', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('soon', 'RB'),\n",
       " ('bear', 'JJ'),\n",
       " ('fruit', 'NN'),\n",
       " (',', ','),\n",
       " ('otherwise', 'RB'),\n",
       " ('we', 'PRP'),\n",
       " (\"'ll\", 'MD'),\n",
       " ('have', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('run', 'VB'),\n",
       " ('from', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('bear', 'NN')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Bear with me, this effort with soon bear fruit, \n",
    "          otherwise we'll have to run from the bear\"\"\"\n",
    "\n",
    "tokenized_words = word_tokenize(text)\n",
    "tagged_words = nltk.pos_tag(tokenized_words)\n",
    "tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DT'),\n",
       " ('bird', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('hand', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('worth', 'JJ'),\n",
       " ('two', 'CD'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('bush', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Good', 'JJ'),\n",
       " ('things', 'NNS'),\n",
       " ('come', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('those', 'DT'),\n",
       " ('who', 'WP'),\n",
       " ('wait', 'VBP'),\n",
       " ('.', '.'),\n",
       " ('There', 'EX'),\n",
       " ('are', 'VBP'),\n",
       " ('other', 'JJ'),\n",
       " ('fish', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('sea', 'NN'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('ball', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('in', 'IN'),\n",
       " ('your', 'PRP$'),\n",
       " ('court', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"A bird in hand is worth two in the bush. \" +\\\n",
    "       \"Good things come to those who wait. \" +\\\n",
    "       \"There are other fish in the sea. \" +\\\n",
    "       \"The ball is in your court.\"\n",
    "\n",
    "tokenized_words = word_tokenize(text)\n",
    "tagged_words = nltk.pos_tag(tokenized_words)\n",
    "tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 7),\n",
       " ('DT', 5),\n",
       " ('IN', 4),\n",
       " ('.', 4),\n",
       " ('JJ', 3),\n",
       " ('VBP', 3),\n",
       " ('VBZ', 2),\n",
       " ('CD', 1),\n",
       " ('NNS', 1),\n",
       " ('TO', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "fd = FreqDist(tagged_words)\n",
    "fd_tagged = FreqDist(tag for (word, tag) in tagged_words)\n",
    "fd_tagged.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University. This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on. 1.1 gives an example of each genre (for a complete list, see http://icame.uib.no/brown/bcm-los.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/loonycorn/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.brown.words()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lexical categories like \"noun\" and part-of-speech tags like NN seem to have their uses, but the details will be obscure to many readers. You might wonder what justification there is for introducing this extra level of information. Many of these categories arise from superficial analysis the distribution of words in text. Consider the following analysis involving woman (a noun), bought (a verb), over (a preposition), and the (a determiner). The text.similar() method takes a word w, finds all contexts w1w w2, then finds all words w' that appear in the same context, i.e. w1w'w2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similar words here belong to the same part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man time day way girl year house people world city family state room\n",
      "country car woman program church government job\n"
     ]
    }
   ],
   "source": [
    "text.similar('boy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get be do in see work go have take make put and find time look day say\n",
      "use come show\n"
     ]
    }
   ],
   "source": [
    "text.similar('run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in on to of and for with from at by that into as up out down through\n",
      "is all about\n"
     ]
    }
   ],
   "source": [
    "text.similar('over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
